---
title: Machine learning-assisted treatment selection for smoking cessation
author:
  - name: Gaylen E Fronk
    orcid: 0000-0001-6653-9699
    corresponding: false
    roles: []
    affiliations:
      - Department of Psychology, University of Wisconsin-Madison
  - name: Kendra Wyant
    orcid: 
    corresponding: false
    roles: []
    affiliations:
      - Department of Psychology, University of Wisconsin-Madison
  - name: Megan E Piper
    orcid: 
    corresponding: false
    roles: []
    affiliations:
      - Center for Tobacco Research & Intervention, University of Wisconsin-Madison
  - name: Timothy Baker
    orcid: 
    corresponding: false
    roles: []
    affiliations:
      - Center for Tobacco Research & Intervention, University of Wisconsin-Madison
  - name: John J. Curtin 
    orcid: 0000-0002-3286-938X
    corresponding: true
    email: jjcurtin@wisc.edu
    roles: []
    affiliations:
      - Department of Psychology, University of Wisconsin-Madison 
keywords:
  - Substance use disorders  
  - Precision mental health  
  - Cigarette smoking  
  - Machine learning  
  - Treatment selection  
abstract: |
  Precision mental health seeks to select the right treatment for a patient given personal characteristics. The purpose of this project was to build a machine learning model that could select among first-line medication treatments for cigarette smoking. We used data from a previously completed comparative effectiveness trial in which participants were richly characterized at baseline before being randomly assigned to varenicline, combination nicotine replacement therapy, or nicotine patch. We built a model predicting treatment success (abstinent vs. smoking) using baseline characteristics and their interactions with treatment. Models were fit, selected, and evaluated using nested cross-validation and the performance metric area under the receiving operator characteristic curve (auROC). Our best models had a median auROC of 0.69 in held-out test sets. We used this model to calculcate probabilities of treatment success for each participant on each of the three treatments to identify their model-predicted best treatment. Individuals who were matched to their model-predicted best treatment during the original trial were more likely to quit successfully at 4 weeks than individuals who were not (OR=1.382, p=0.014) but not at later timepoints. This project produces a clinically implementable treatment selection model to assist people quitting cigarette smoking. 
key-points:
  - Take away point 1 
  - Take away point 2
date: last-modified
bibliography: references.bib
bibliographystyle: apa 
number-sections: false 
editor_options: 
  chunk_output_type: console
---

<!--nts: abstract 187 words -->

## Introduction


Precision medicine seeks to guide and personalize treatment selection using individual difference characteristics that are likely to predict treatment success for each patient [@bickmanImprovingMentalHealth2020]. Successful precision medicine would increase the likelihood of treatment success for each patient because each patient receives the treatment predicted to work best for them. It would also improve treatment effectiveness rates across the population because each treatment is administered only to the patients for whom that treatment is expected to be their best option. 

Precision mental health is the application of the precision medicine paradigm to mental health conditions [@derubeisHistoryCurrentStatus2019; @inselNIMHResearchDomain2014; @bickmanAchievingPrecisionMental2016; @bickmanImprovingMentalHealth2020]. Precision mental health models may allow us to select among these treatments to improve mental healthcare. Treatments for mental health conditions are usually no more than moderately effective, and many treatments for the same disorder are comparably effective [@lewisPsychologicalTherapiesPosttraumatic2020; @weiszArePsychotherapiesYoung2019; @adjeiCO110ComparativeEffectiveness2022]. Consequently, many researchers have pursued precision mental health in recent years [@derubeisHistoryCurrentStatus2019]. 

<!--PAI uses more than one feature though.  Need to think about how to both acknowledge that -- because its maybe the most salient current approach for PMH-- again because we would like ask for both derubeis and cohen to be reviewers-->
Despite the abundance of research, much less progress has been made to personalize treatments for mental health disorders compared to treatments for medical disorders [@bickmanAchievingPrecisionMental2016; @bickmanImprovingMentalHealth2020; @kranzlerPrecisionMedicinePharmacogenetics2017; @oliverPrecisionMedicineAddiction2017; @kesslerPragmaticPrecisionPsychiatry2021]. One explanation is that many factors influence heterogeneous, complex clinical phenomena like mental health diagnoses and treatment success [@feczkoMethodsChallengesAssessing2020]. Thus, any single feature (i.e., predictor variable) cannot account for more than a small portion of the variance in treatment success [@kesslerPragmaticPrecisionPsychiatry2021; @inselNIMHResearchDomain2014]. Precision mental health efforts so far, however, have largely focused on personalizing treatments using only a single feature [@derubeisHistoryCurrentStatus2019]. It is perhaps unsurprising that models that consider only one or a small handful of features - which also limits considering features from different categories concurrently - have failed to capture the real-world complexity underlying clinical phenomena like treatment success.

<!--JJC: Likely not very overfit if only one feature.  This would be the first problem that would be encountered if the field tried to increase features without changing analytic strategy to ML.   Current models are crappy not because of  high variance but likely high bias b/c only one feature of many from the DGP.  This latter point --problem with bias not variance/overfitting-- isnt worth making because for this audience.  Made just to anchor this for you.-->
Additionally, because models are typically developed and evaluated in the same sample, the models may become very overfit to that sample [@jonathanUseCrossvalidationAssess2000]. This problem is exacerbated in precision mental health because sample sizes in psychological research have remained relatively small despite recommendations to increase sample size [@marszalekSampleSizePsychological2011]. Consequently, precision mental health models rarely generalize well to new patients. 

<!-- You can combine these two paragraphs to be more accurate.  Need more but can't do it without overfitting.-->
To capture sufficient complexity to predict treatment success, we need to increase the total number of features in precision mental health models. Incorporating more features, however, makes overfitting the data more likely. Thus, successful precision mental health requires an analytic approach that can handle high-dimensional data without becoming too overfit to generalize to new patients.

### Applying machine learning approaches

<!--OK, this paragraph could acknowledge how these techniques reduce bias while controlling overfitting if you did introduce both bias and overfitting (could use overfitting as the main term but acknowledge it produces models that perform poorly because of high variance - or maybe not even bring in variance.  not sure.  But this paragraph otherwise suggests that the main problem in PMH has been overfitting, which will be addressed with these techniques.   I guess another pivot would be to anchor this back to revised paragraph above where you have made clearer that WHEN the field adds more features, those models would have an overfitting problem and ML can reduce the overfitting in those model.   I think you are mostly there on these "small" issue but a little more tuning on one or the other of these strategies will help a bit.-->
Machine learning may be able to advance precision mental health goals [@bickmanAchievingPrecisionMental2016; @dwyerMachineLearningApproaches2018; @maceachernMachineLearningPrecision2021; @mooneyBigDataPublic2018]. These models use statistical algorithms trained on high-dimensional arrays (hundreds or even thousands) of features [@jamesIntroductionStatisticalLearning2013; @kuhnAppliedPredictiveModeling2018; @ngMachineLearningYearning2018]. Machine learning uses various techniques (e.g., regularization, hyperparameter tuning, simultaneous consideration of many model configurations) within resampling methods such as cross-validation to accommodate high-dimensional sets of features while reducing overfitting [@jamesIntroductionStatisticalLearning2013; @kuhnAppliedPredictiveModeling2018; @ngMachineLearningYearning2018; @mooneyBigDataPublic2018; @krstajicCrossvalidationPitfallsWhen2014; @jonathanUseCrossvalidationAssess2000]. Thus, we can build precision mental health models that capture complex clinical phenomena and generalize accurately to new data. 

### Cigarette smoking as a critical precision mental health target


Combining precision mental health and machine learning could improve treatment for cigarette smoking. Smoking remains an enormous public health burden. Tobacco is the leading cause of preventable death in the U.S., accounting for more than 480,000 deaths annually [@nationalcenterforchronicdiseasepreventionandhealthpromotionusofficeonsmokingandhealthHealthConsequencesSmoking2014; @schlamInterventionsTobaccoSmoking2013; @corneliusTobaccoProductUse2020]. Individuals who smoke have two- to three-fold likelihood of death across causes and lose over a decade of life expectancy [@jhaprabhat21stCenturyHazardsSmoking2013]. Although rates of smoking have declined considerably, approximately 14% of U.S. adults continue to smoke daily or near-daily [@corneliusTobaccoProductUse2020; @substanceabuseandmentalhealthservicesadministrationKeySubstanceUse2023]. Additionally, cigarette smoking rates remain much higher in potentially vulnerable populations. This includes people with chronic or severe mental illness including other substance use disorders; Native American and non-Hispanic Black individuals; individuals who are economically and educationally disadvantaged; people in the criminal legal system; people experiencing homelessness; individuals who are uninsured or insured through Medicaid; and people who identify as lesbian, gay, or bisexual [@bakerSmokingTreatmentReport2021; @jamalCurrentCigaretteSmoking2015a; @corneliusTobaccoProductUse2020; @kellyPrevalenceSmokingOther2012; @cropseySmokingFemalePrisoners2004; @harrisonCigaretteSmokingMental2020; @baggettTobaccoUseHomeless2013; @soarSmokingAmongstAdults2020].

The best available smoking cessation treatments are modestly effective. The medications varenicline and combination nicotine replacement therapy (C-NRT) are the most effective options when combined with psychosocial counseling, yielding 6-month treatment success rates of 30-35% [@cahillPharmacologicalInterventionsSmoking2013; @schlamInterventionsTobaccoSmoking2013; @rigottiTreatmentTobaccoSmoking2022; @fioreClinicalPracticeGuideline2008; @bakerSmokingTreatmentReport2021]. Varenicline and C-NRT appear to be equally effective  [@cahillPharmacologicalInterventionsSmoking2013; @bakerEffectsNicotinePatch2016]. Indeed, national clinical guidelines note “there are no well-accepted algorithms to guide optimal selection” between any of the first-line medications ([@fioreClinicalPracticeGuideline2008], p. 44).<!--KW: not sure how to format reference with page number but double (( are being rendered as is. I wonder if [@fioreClinicalPracticeGuideline2008, p. 44] would work? GEF: known issue, will follow up with Susan about if she has found a solution. may just need to manually edit in word for submission --> These facts suggest a critical need for precision mental health approaches in the cigarette smoking domain.

#### Previous research


To select among treatments for smoking cessation, precision mental health models must include *many features* and *multiple treatments* as inputs. Using a high-dimensional array of features is necessary to predict a complex clinical outcome like treatment success. Including multiple treatments allows for interactions among treatments and features that are needed to make differential predictions by treatment. 

There is a large body of research identifying features that predict overall treatment success (i.e., irrespective of treatment) [@laiDevelopmentMachineLearning2021; @kaufmannRateNicotineMetabolism2015; @kayeSearchingPersonalizedMedicine2020a; @piperPrecisionSmokingCessation2017a; @issabakhshMachineLearningApplication2023; @etterPredictingSmokingCessation2023] or treatment success when using a single treatment [@massagoApplicabilityMachineLearning2024; @coughlinMachineLearningApproachPredicting2020a]. However, these models that do not include multiple treatments do not offer an actionable way forward to select among treatment options. Even research that shows who might succeed within a specific treatment has limited utility for treatment selection: what does this mean for a patient who is not predicted to succeed using that treatment? 

Some research has begun to build models that consider multiple treatments simultaneously [@chenGeneticVariantCHRNA52020; @shahabDoesNicotineMetabolite2019; @schnollNicotineMetabolicRate2009; @glatardAssociationNicotineMetabolism2017; @chenowethNicotineMetaboliteRatio2016; @lermanUseNicotineMetabolite2015; @siegelUseNicotineMetabolite2020; @kayeSearchingPersonalizedMedicine2020a; @piperIdentifyingEffectiveIntervention2016; @piperPrecisionSmokingCessation2017; @piperPrecisionSmokingCessation2017a]. However, each study examined only a single feature or examined each feature in a separate model, which cannot explain sufficient variance in complex clinical outcomes. 

#### Opportunities for treatment selection

<!--the "still" seems misplaced and it doesn't place the emphasis on the point that they don't differ overall but still might differ for specific individuals-->
There may still be reason to expect that some smoking cessation treatments work better than others for a specific individual. First, there is enormous heterogeneity among people who smoke cigarettes [@oliverPrecisionMedicineAddiction2017; @wangSociodemographicVariabilityAdolescent2009; @zhengIdiographicExaminationDaytoDay2013]. Second, smoking cessation medications have distinct pharmacological mechanisms of action that may affect how helpful they are for different people [@cahillNicotineReceptorPartial2016; @jordanDiscoveryDevelopmentVarenicline2018; @liebermanDopaminePartialAgonists2004]. Third, the many behavioral and environmental features that have been shown to predict overall treatment success may also guide treatment selection, alone or in combination with medication mechanisms of action [@bickelPredictorsSmokingCessation2023]. Finally, there is some evidence of differential effectiveness, adherence, and preference for treatments across individuals [@heckmanEffectivenessSwitchingSmokingCessation2017; @fioreClinicalPracticeGuideline2008; @tonnesenRecyclingNicotinePatches1993; @gonzalesRetreatmentVareniclineSmoking2014; @cropseyPilotTrialVivo2017; @lindsonDifferentDosesDurations2019]. 

## Purpose


The goal of this project was to evaluate the clinical benefit of a treatment selection model. Specifically, we assessed whether using our model to select among medications for smoking cessation can increase abstinence following quitting smoking. We also assessed whether any treatment benefit is fair such that it performs equitably across demographic groups. To increase confidence in any clinical benefit we observed, we also evaluated model performance (i.e., is there predictive signal?) and interpreted our model (i.e., how does our model work?).

<!--That is a GREAT intro.  Unfortunately, I don't see much opportunity to cut much.  Its clear, compelling and already very focused!-->
## Methods

### Transparency & openness


We adhere to research transparency principles that are crucial for robust and replicable science. We reported how we determined the sample size, all data exclusions, all manipulations, and all study measures. We provide a transparency report in the supplement <!-- NTS: Kendra did this for us, GEF to review -->. Finally, our data and other study materials are publicly available on our [OSF page](https://osf.io/qad4n/), and our annotated analysis scripts and results are publicly available on our [study website](https://jjcurtin.github.io/study_match/). 
<!--  does this site include the pre-reg we didnt follow? GEF: the OSF page does. need to think about how to give access to data maybe without linking to OSF -->
<!--JJC:  Maybe something like this:  "We pre-registered our (hypothesis or research questions, or aims - can't remember) and proposed analyses.  We also clearly indicate subsequent modifications to our pre-registered analyses with the justification for the departure."-->

### Data


Our data come from a completed randomized controlled trial conducted by the University of Wisconsin (UW) Center for Tobacco Research and Intervention (CTRI) [@bakerEffectsNicotinePatch2016]. This trial compared the effectiveness of three cigarette smoking cessation treatments: varenicline, combination nicotine replacement therapy (C-NRT), and nicotine patch. Briefly, 1086 adults who smoked cigarettes daily and were looking to quit smoking were enrolled in Madison, WI, USA and Milwaukee, WI, USA. Smoking was biochemically verified. Participants were smoking approximately 17 cigarettes per day (*M*=17.03, *SD*=8.31) at study enrollment. Exclusion criteria included contraindicated medical (e.g., severe hypertension) or psychiatric (e.g., severe and persistent mental illness) conditions; current use of contraindicated medications; and pregnancy or unwillingness to use contraception while taking a study medication. Participants set a quit date with study staff and were enrolled for several weeks prior to the target quit date through at least 6 months following quitting smoking. 

#### Treatment conditions


Participants were randomly assigned to one of three medication conditions: varenicline, C-NRT, or nicotine patch. For varenicline, participants began medication use prior to their quit attempt, starting with 0.5 mg once daily for 3 days, followed by 0.5 mg twice daily for 4 days. On the target quit date, participants increased to 1 mg twice daily and continued this dose for 12 weeks except in response to adverse events. For C-NRT or nicotine patch, participants initiated treatment on their quit date. Patch dosing was 8 weeks of 21 mg, 2 weeks of 14 mg, and 2 weeks of 7 mg. All individuals who received C-NRT were also instructed to use at least 5 lozenges per day (2 or 4 mg nicotine lozenges determined by time to first daily cigarette) for the full 12 weeks except in the case of adverse effects.

All participants also received counseling per clinical guidelines [@fioreClinicalPracticeGuideline2008]. Participants received 5 in-person counseling sessions (pre-quit, quit day, week 1, week 4, week 12) and 1 phone counseling session (week 8). Counseling consisted of motivational interviewing/enhancement, supportive therapy, and skill training.

#### Individual difference characteristics

<!--interesting hack on the tables.   Curious to hear more about how this works.  Are you using different R functions/packages for the two table options?-->
<!--Does  this table count against words.  Should it be in appendix?  how much do the individual features fit into the story your are telling or the discussion of results?-->
Participants were comprehensively assessed for individual difference characteristics prior to treatment randomization. These characteristics fall into several domains expected to relate to cigarette smoking cessation: tobacco-related (e.g., cigarettes per day), psychological (e.g., psychiatric diagnoses, distress tolerance), physical health (e.g., body mass index), social/environmental (e.g., living with another person who smokes), and demographic (e.g., age, sex). A detailed list of all available individual differences characteristics appears in [@tbl-featureshtml]{.content-visible when-format="html"}[@tbl-featurespdf]{.content-visible unless-format="html"}.

::: {.content-hidden unless-format="html"}

{{< embed notebooks/mak_tables.qmd#tbl-featureshtml >}} 
:::

::: {.content-hidden when-format="html"}

{{< embed notebooks/mak_tables.qmd#tbl-featurespdf >}} 
{{< pagebreak >}}   
 
[Table 1: Individual Differences (Continued)]{.center}
{{< embed notebooks/mak_tables.qmd#featurespdf2 >}} 

{{< pagebreak >}}  
[Table 1: Individual Differences (Continued)]{.center}
{{< embed notebooks/mak_tables.qmd#featurespdf3 >}} 

{{< pagebreak >}}  

[Table 1: Individual Differences (Continued)]{.center}
{{< embed notebooks/mak_tables.qmd#featurespdf4 >}} 
:::

#### Treatment success outcome

Throughout study participation, treatment success was repeatedly assessed via biologically confirmed, 7-day point-prevalence abstinence. Point-prevalence abstinence assesses for any smoking (here, in the previous 7 days) and yields a single, dichotomous outcome of "abstinent" or "smoking." Participants self-reported whether they had smoked over the past 7 days, and their report was biologically confirmed via exhaled carbon monoxide (CO). Participants were labeled as "abstinent" if their CO level was less than 6 parts per million (ppm; [@bakerEffectsNicotinePatch2016]). If participants self-reported any smoking in the past 7 days, their CO level contradicted their self-report (i.e., CO level > 6 ppm), or biological confirmation could not be completed, participants were labeled as "smoking." 

### Evaluating clinical benefit


The primary purpose of this project was to evaluate the clinical benefit of our treatment selection model. In other words, we determined whether using a model to select treatments for individual patients improves clinical outcomes. Details on fitting, selecting, and evaluating model performance of the treatment selection model appear in "Model building, selection, & evaluation."

#### Identify model-predicted best treatment

<!--This is a bit confusing if you arent a machine learning person thinking about cross validation.  Maybe a sentence or two more.  Is it worth also making clear this is a standard way to get held-out predictions - LOOCV?  Is it worth stating that Derubeis/cohen did this in their work?-->
We used our best model configuration (i.e., combination of model characteristics; see Model Building, Selection, & Evaluation) to make treatment success predictions for each participant. Models were fit with 1085 participants, and predictions were made for the remaining, held-out participant. This approach matches real-world implementation in that we made predictions for a new patient the model has not seen. 

We calculated three predicted probabilities for each participant by substituting each treatment into the model inputs. This produced one prediction per person per treatment. The treatment that yielded the highest model-predicted probability of abstinence was identified as that participant's "best" treatment. 

For example, an individual received varenicline in the original trial. We calculated their probability of abstinence using their data (i.e., with varenicline as the "treatment" feature), and we calculated two additional probabilities by substituting C-NRT and nicotine patch for varenicline. If their probability of abstinence was highest when substituting C-NRT, C-NRT was identified as their best treatment.

#### Categorize treatment matching


Some participants' best treatment matched what they were randomly assigned in the original trial. Other participants may have received a sub-optimal treatment (i.e., what the model identified as their second-best or worst treatment based on calculated probabilities). Thus, participants could be categorized as being matched to "best treatment" or "other treatment" in the original trial. 

For example, the individual described above received varenicline in the original trial, but their model-predicted probability of abstinence was highest when substituting C-NRT for treatment. This participant's best treatment did not match their trial treatment, so they were labeled as matched to "other treatment."

#### Analysis plan


Our primary analysis evaluated the clinical benefit of our treatment selection model by comparing the observed outcomes (i.e., abstinence vs. smoking from the original trial) for people who were matched to their best treatment or to other treatment (between-subjects; 0.5 ["best treatment"] vs. -0.5 ["other treatment"]). 

<!--JJC: you could say here (in footnote if footnotes allows) that we planned initially to include interactions but then say why they are not optimal.  And we can report the interactive models still in the supplement if we don't-->
<!--  JJC: This is a bit more accurate description of the problem (but feel free to edit further for increased clarity) because we don't build models with different link function.  Logistic only uses the logit link function.  "The interpretation of interactions in logistic models is not straightforward because an inferential test of the parameter estimate for an interaction evaluates the effect in log-odds units but we commonly interpret our effects in units of probability or odds.  Transformations across these units is not linear and significant effects in log-odds may not translate to similar patterns for probability or odds [@karaca-mandicInteractionTermsNonlinear2012; @collinsOptimizationBehavioralBiobehavioral2018].--> 
We examined this effect of treatment matching on abstinence at 4, 12, and 26 weeks post-quit. Calculating and interpreting interactions in logistic models is not straightforward because significance can differ based on the link function used and whether effects are determined in probabilities, odds, or log-odds units [@karaca-mandicInteractionTermsNonlinear2012; @collinsOptimizationBehavioralBiobehavioral2018]. Consequently, we conducted three, separate generalized logistic regression models where we regressed the outcome (abstinent vs. smoking) on treatment matching. We conducted these regressions using the lme4 package in R [@batesLme4LinearMixedEffects2015]. We also conducted sensitivity analyses using only participants who reported starting their medication (defined as any medication use reported during the first 4 weeks post-quit; N=988).

#### Fairness benchmarks


Treatment selection models that work for only a subset of people, if implemented, could widen existing mental healthcare disparities. Consequently, we conducted fairness analyses to determine whether there was equitable clinical benefit across demographic groups. We conducted analyses separately for three dichotomized demographic groups that display higher rates of cigarette smoking and/or encounter additional barriers to receiving treatment: 1) non-Hispanic White vs. non-White participants; 2) individuals with income above vs. below the poverty line; and 3) male vs. female participants [@jamalCurrentCigaretteSmoking2015a; @corneliusTobaccoProductUse2020; @pinedoCurrentReexaminationRacial2019; @olfsonHealthcareCoverageService2022; @kilaruIncidenceTreatmentOpioid2020].

For any timepoints (4, 12, or 26 weeks) for which there was a significant benefit of treatment matching, we compared the effect across demographic groups. Given the issues with interpreting interactions in logistic regression models described above, we examined only simple effects of treatment matching for each pair of demographic groups (e.g., simple effect of treatment matching for non-Hispanic White and for non-White individuals) and did not interpret *p*-values.  <!--JJC: i want to return to this after I read results-->

### Model building, selection, & evaluation


Data preprocessing, modeling, and Bayesian analyses were conducted in R using the tidymodels ecosystem [@kuhnTidymodelsCollectionPackages2020]. Models were trained and evaluated using high-throughput computing resources in the University of Wisconsin Center for High Throughput Computing [@chtc]. Additional details about model building and evaluation appear in the Supplement.

#### Feature engineering


We followed standard practices for generic feature engineering steps (see Supplement). Treatment was one-hot-coded such that there were three features, one corresponding to each treatment (varenicline, C-NRT, nicotine patch). The three treatment features were allowed to interact with all other features to permit differential prediction. A sample feature engineering script (i.e., tidymodels recipe) containing all feature engineering steps is available on our OSF study page ([https://osf.io/qad4n/](https://osf.io/qad4n/)). <!-- GEF: move this to study website if hoping to "cut" links to OSF page -->

#### Model configurations

<!--JJC:  it is a bit more complicated than that glmnet allows explicit interactions.  The key is it allowed us to look at interactions for feature importance and use a satisfying criterion that X interaction terms were retained. We could have used random forest or xgboost and still got an interactive model but would couldnt have pushed it to retain intearctions terms.  In fact, I think we earlier tried random forest and it didnt produce differential models but it COULD have, right?-->
All model configurations used the statistical algorithm Elastic Net Logistic Regression (GLMNet) and differed across sensible values for the associated hyperparameters, alpha and lambda. This algorithm aligned with our application goals: GLMNet allows including interaction terms explicitly (i.e., interactions between treatment and all other features), and it penalizes model complexity to reduce dimensionality and ultimately reduce the risk of overfitting. Model configurations differed by 1) whether feature sets included individual items within a self-report measure or total scale/sub-scale scores derived from items, and 2) whether ordered categorical data (e.g., Likert scale items) were ordinal scored or one-hot coded. 

#### Prediction outcome


The model prediction outcome was treatment success at 4 weeks post-quit (i.e., predicting if individuals were labeled "abstinent" or "smoking" at 4 weeks). We selected treatment success at 4 weeks in order to 1) predict an outcome that - in clinical practice - would allow individuals to pivot to a different medication earlier if their treatment was unsuccessful; and 2) evaluate the differential effects of treatment while participants were actively using the medications.^[We fit additional models predicting treatment success at 12 and 26 weeks, and these models had significantly worse performance than our primary 4 week prediction model (See Supplement).]

#### Model selection


We used nested *k*-fold cross-validation for model training, selection, and evaluation [@krstajicCrossvalidationPitfallsWhen2014]. Nested cross-validation uses two nested loops for dividing and holding out folds: an outer loop, where held-out folds serve as *test sets* for model evaluation; and inner loops, where held-out folds serve as *validation sets* for model selection. We used 1 repeat of 10-fold cross-validation (i.e., *k*=10) for the inner loops and 3 repeats of 10-fold cross-validation for the outer loop.

<!--JJC:  maybe we need to be a little more clear on optimizing vs. satisfying criteria and use that latter term?  Otherwise, I worry people my think we are doing something funky here.  Maybe: " Models were selected and evaluated using area under the receiver operating characteristic curve (auROC), which indexes the probability that the model will predict a higher score for a randomly selected positive case (abstinent) relative to a randomly selected negative case (smoking) [@kuhnAppliedPredictiveModeling2018; @youngstromPrimerReceiverOperating2014]. Because it was important that our models retained interaction features, we considered only a subset of model configurations that first met a "satisfying criterion" [@ngMachineLearningYearning2018] such that they retained an average of 50 or more treatment interaction features --> 
Models were selected and evaluated using area under the receiver operating characteristic curve (auROC), which indexes the probability that the model will predict a higher score for a randomly selected positive case (abstinent) relative to a randomly selected negative case (smoking) [@kuhnAppliedPredictiveModeling2018; @youngstromPrimerReceiverOperating2014]. Because it was important that our model retained interaction features, we required that model configurations retain an average of 50 or more treatment interaction features [@ngMachineLearningYearning2018]. Best model configurations were selected using median auROC among model configurations that retained a median of 50 treatment interaction features across the 10 *validation sets*. Final performance evaluation of those best model configurations used median auROC across the 30 *test sets*.

To get a single, final model configuration, we replicated our inner loop resampling (1 repeat of 10-fold cross-validation) on the full dataset. The best model configuration was selected using median auROC across the 10 held-out folds (among model configurations that retained a median of 50 treatment interaction features). This best model configuration was used to make predictions for clinical benefit evaluation (above) and feature importance analyses (see Model Interpretation). A final model was fit on the full dataset using this best configuration to obtain parameter estimates for additional interpretation (see Model Interpretation).

#### Model performance evaluation


We used a Bayesian hierarchical generalized linear model to estimate the posterior probability distributions and 95% Bayesian credible intervals (CIs) for test set auROC. We estimated the posterior probability distribution around model performance following recommendations from the tidymodels team [@kuhnTidyposteriorBayesianAnalysis2022]. The median posterior probability for auROC represents our best estimate for the magnitude of the auROC parameter for each model. If the credible intervals do not contain 0.5 (chance performance), this <!--JJC: provides strong evidence that -->suggests our model is capturing signal in the data. 

<!--KW: John had me add a little more information that we had moved to supplement for EMA paper when describing the Bayesian model. I updated your last paragraph to include this below. However, depending on word constraints you might need to move some of the info to supplement? Also, for the two new citations introduced you already have them in the paper_match zotero library!

We used a Bayesian hierarchical generalized linear model to estimate the posterior probability distributions and 95% Bayesian credible intervals (CIs) for test set auROC. Following recommendations from the rstanarm team and others [@rstudioteamRStudioIntegratedDevelopment2020; @gabryPriorDistributionsRstanarm2023], we used the rstanarm default autoscaled, weakly informative, data-dependent priors that take into account the order of magnitude of the variables to provide some regularization to stabilize computation and avoid over-fitting.^[Priors were set as follows: residual standard deviation ~ normal(location=0, scale=exp(2)), intercept (after centering predictors) ~ normal(location=2.3, scale=1.3), the two coefficients for window width contrasts ~ normal (location=0, scale=2.69), and covariance ~ decov(regularization=1, concentration=1, shape=1, scale=1).] We set two random intercepts to account for our resampling method: one for the repeat, and another for the fold nested within repeat. The median posterior probability for auROC represents our best estimate for the magnitude of the auROC parameter for each model. If the credible intervals do not contain .5 (chance performance), this suggests our model is capturing signal in the data. 

GEF: I believe all this info is in the supplement but will confirm, thank you!
-->

#### Model calibration

<!--JJC: Do we report this is Supplement?-->
We examined the calibration of our model's predictions by comparing model predictions to observed outcomes. Specifically, we compared the predicted probabilities for each individual for their original trial-assigned treatment against the observed trial treatment success. 

### Model interpretation


Interpretability is important when using machine learning for clinical applications to identify important features and encourage implementation [@maceachernMachineLearningPrecision2021; @mooneyBigDataPublic2018; @cohenTreatmentSelectionDepression2018]. We reviewed the retained features and their parameter estimates as a metric of feature importance because GLMNet only retains features whose contribution outweighs the additional complexity. 

We also computed Shapley Values [@lundbergUnifiedApproachInterpreting2017] to provide a consistent, objective explanation of feature importance. Shapley values were calculated for each held-out participant using a model fit with the other 1085 participants and the final, best selected model configuration. We used the DALEX and DALEXtra packages [@biecekDALEXExplainersComplex2018] in R, which provide local Shapley values (i.e., for each observation) in log-odds units for binary classification models. To index global feature importance, we averaged the absolute value of the local Shapley values of each feature across observations. Shapley values are additive, which allowed us to create two feature categories, Main Effects and Interactions. We calculated global importance of each feature category by averaging the absolute value of the Shapley values of all features in the category across observations. These global importance scores allowed us to contextualize relative feature importance for each feature and feature category.

## Results

### Sample characteristics


All 1086 participants who were randomized to treatment in the comparative effectiveness trial [@bakerEffectsNicotinePatch2016] were included in our analysis sample. Demographic characteristics of this sample appear in [@tbl-demohtml]{.content-visible when-format="html"}[@tbl-demopdf]{.content-visible unless-format="html"}. Smoking-related characteristics of this sample appear in @tbl-smoking-chars.

::: {.content-hidden unless-format="html"}

{{< embed notebooks/mak_tables.qmd#tbl-demohtml >}} 
:::

::: {.content-hidden when-format="html"}

{{< embed notebooks/mak_tables.qmd#tbl-demopdf >}} 
{{< pagebreak >}}   
 
[Table 2: Demographic Characteristics (Continued)]{.center}
{{< embed notebooks/mak_tables.qmd#demopdf >}} 
:::

{{< embed notebooks/mak_tables.qmd#tbl-smoking-chars >}} 

### Clinical benefit


There was a significant effect of treatment matching on abstinence at 4 weeks (OR=1.382, *z*=2.452, *p*=0.014) such that individuals who received their model-predicted best treatment were more likely to be abstinent. The effect of treatment matching was no longer significant at 12 weeks (*p*=0.232) or at the 26-week follow-up assessment (*p*=0.943). @fig-clin-ben-wk4 shows the mean abstinence rate by treatment matching at each time point. The pattern of results was identical when including only individuals who reported any medication use in the analysis sample (4 weeks: OR=1.359, *z*=2.257, *p*=0.024; 12 weeks: *p*=0.295; 26 weeks: *p*=0.902). <!--footnote for this latter result?  Was that analysis pre-registered?  I want to report it but we need to be clear on pre-registered or not if we will link to pre-reg.-->

{{< embed notebooks/eval_benefit_4wk.qmd#fig-clin-ben-wk4 >}}

We conducted follow-up fairness analyses for the significant effect of treatment matching at 4 weeks. *Figure X* shows the mean abstinence rate for individuals matched to their best treatment and individuals matched to other treatment separately by sex (Panel A), race/ethnicity (Panel B), and income (Panel C). The treatment matching effect size was similar for male (OR=1.395) and female (OR=1.372) participants, and abstinence rates were similar across groups. Non-White participants had a descriptively greater benefit of treatment matching (OR=2.164) than non-Hispanic White participants (OR=1.208). However, abstinence rates overall were lower for non-White participants. The treatment matching benefit was similar for individuals whose income was above (OR=1.437) and below (OR=1.529) the national poverty line, but abstinence rates were lower overall for individuals below the poverty line.^[Fifty-three participants (4.9%) did not provide income data and were excluded from this analysis. As a comparison, the overall effect of treatment matching for this sample subset was OR=1.482, *z*=2.712, *p*=0.007.] <!--JJC: when rates were different by income, you reported those rates separate. but you didnt do that for race/ethnicity.  Am i missing the reason why?  Also, you are reporting odds ratios in this paragraph.   Should the rates then be reported in odds to be consistent?  I suspect we do probabilities in other results?   Should everything be in odds and odds ratios?  Odds are reported for treatment outcome in some papers though I think probability is probability is probably better understood-->

<!-- {{< embed notebooks/ana_fairness_benefit.qmd#fig-clin-ben-fair >}} -->


### Model performance

<!-- JJC: bit of a hack but want to round to 2 decimal places for auROCs to get .70? Or does it not round to . 70?-->
The median auROC across the 30 test sets for the 4-week model was 0.695 (IQR=0.667-0.718, range=0.592-0.788). @fig-combined, Panel A shows the ROC curve for held-out test set performance (concatenated across 30 held-out folds).

{{< embed notebooks/mak_fig_1.qmd#fig-combined >}}

We used the 30 test set auROCs to estimate the posterior probability distribution for the auROC of these models. The median auROC from the posterior distribution was 0.693. This value represents our best estimate for the magnitude of the auROC parameter. The 95% Bayesian CI for the auROC was relatively narrow [0.674-0.711] and did not contain 0.5 (chance performance), providing strong evidence that this model has predictive signal. @fig-combined, Panel B displays the posterior probability distribution for the auROC.

@fig-combined, Panel C displays model calibration. Predicted probabilities are binned (bin width=10%) and plotted against the observed probability of abstinence for observations in that bin. If probabilities were perfectly calibrated, all bin means would fall on the dotted line (e.g., bin from 0-10 with an observed mean probability of 0.05, bin from 10-20 with an observed mean probability of 0.15). Probabilities are well calibrated and ordinal in their relationship with the true probability of abstinence. 

### Model interpretation


Our final model fit with the full dataset retained 155 features overall including 74 treatment interactions (see *Supplemental Table X* for retained feature list). To perform treatment selection, only interactive features need to be assessed, as features that increase or decrease probabilities equally across treatments do not affect different predictions across candidate medications. Implementing this model for treatment selection requires assessing only 52 unique items (e.g., multiple one-hot features are from a single item, the same feature interacts with more than one treatment). 

Global feature importance (mean |Shapley value|) from our model appears in @fig-shap-global. Shapley values describe the relative importance of these individual features for making predictions. Six of the top 25 most globally important features were treatment interactions. 

{{< embed notebooks/shap_4wk.qmd#fig-shap-global >}}

## Discussion


In this project, we produced a treatment selection model that can offer immediate benefit to individuals looking to quit smoking using several first-line medications. Individuals who received their model-predicted best treatment in the original trial had a mean abstinence rate that was 7.4% higher than individuals who did not (38.9% vs. 31.5%) at 4 weeks. This absolute difference corresponds to a 23.5% *relative* improvement - simply by allocating treatments to the right person. <!--JJC: GOOD!-->

We feel confident in this effect for several reasons. First, we made predictions for each individual to identify their best treatment while they were held-out from model fitting to match how this model will be used in clinical practice (i.e., to make predictions and select a treatment for new patients). Second, our model is capturing predictive signal, as supported by the Bayesian CI around our model's auROC, adding to our trust in the model's prediction outputs. Third, these predictions were well-calibrated such that we can trust their ordinal ranking. This is critical because our treatment selection process relies on the relative *order* (i.e., rank) of predicted probabilities for each person rather than the values themselves. 
  
We can achieve this benefit using an accessible, low-burden assessment. Implementing this treatment selection model would require assessing approximately 50 multiple choice and yes/no questions, which survey research suggests would take 11-12 minutes to complete [@lenznerCognitiveBurdenSurvey2010]. Additionally, because all items are self-report questions, this assessment can be completed remotely (e.g., administered online) and made available to people without access to in-person medical care. This remote capacity is particularly valuable because two treatments in the model (C-NRT, nicotine patch) are widely available over-the-counter, offering scalable implementation when healthcare access is limited. 

This focus on accessibility is especially important given disparities in mental healthcare. Access to treatment is a known barrier in mental healthcare and a contributing factor driving healthcare disparities [@jacobsonDigitalTherapeuticsMental2022]. Cigarette smoking rates remain higher in many marginalized populations [@bakerSmokingTreatmentReport2021; @jamalCurrentCigaretteSmoking2015a; @corneliusTobaccoProductUse2020; @kellyPrevalenceSmokingOther2012; @cropseySmokingFemalePrisoners2004; @harrisonCigaretteSmokingMental2020; @baggettTobaccoUseHomeless2013; @soarSmokingAmongstAdults2020]. Prioritizing accessibility in implementation is a critical first step towards mitigating rather than exacerbating health disparities [@maceachernMachineLearningPrecision2021]. 

We further addressed this goal with our fairness analyses. We found that our treatment selection model may be well-positioned to improve treatment equity: Our model offered comparable benefit for female (vs. male) and lower income (vs. higher income) individuals, and it offered greater relative benefit to non-White (vs. non-Hispanic White) individuals. To reduce and ultimately close the treatment gap between privileged individuals and individuals often disadvantaged in research efforts and mental healthcare, we need tools such as this model that offer *additional* benefit to those disadvantaged demographic groups.

### Improving long-term benefit of treatment selection

Alongside these exciting findings, the clinical benefit that our treatment selection model offers is short-lived. There is no longer statistically significant benefit of treatment matching at 12 weeks, though there is a numeric difference (31.2% abstinence vs. 27.8% abstinence, corresponding to a 12.2% relative improvement), and there is no statistical or numeric difference at all by 6 months. 

Initial treatment success is critical, especially for cigarette smoking where even reducing smoking or quitting for a short period of time can improve health outcomes and life expectancy [@jhaprabhat21stCenturyHazardsSmoking2013]. Additionally, smoking early in a quit attempt can have strong negative consequences: decreased self-efficacy, reduced treatment adherence, and premature treatment cessation [@schlamInterventionsTobaccoSmoking2013]. Selecting a treatment that increases treatment success in early recovery (i.e., at 4 weeks) is therefore necessary - though not sufficient - for long-term success. Nevertheless, we were of course hopeful that treatment matching benefits would endure. However, it is perhaps unsurprising that they do not. There may be several possibilities for the lack of benefit at later assessment points. 

First, it is possible that we could have improved treatment selection by improving the underlying model's predictive performance. Our best estimate of median auROC in held-out data (from the posterior probability distribution) was 0.69, indicating that our model correctly assigns a higher probability to a positive (abstinent) case than a negative (smoking) case 69% of the time. An auROC of 0.7-0.8 is considered "acceptable" [@mandrekarReceiverOperatingCharacteristic2010]. Our model was right on the cusp of this range, but there is room for improvement. 

Capturing some signal with our model was necessary to yield credible predictions and establish the relevance of features, and so our model's modest predictive performance may have impacted our ability to select treatments. However, overall prediction of treatment success was not our primary goal and is insufficient for treatment selection. A model that predicts treatment success perfectly (i.e., auROC=1.0) but includes no interactions is useless for treatment selection. Indeed, this fact motivated requiring a minimum number of interaction terms retained in our models, even though it is possible that we sacrificed some level of predictive power by imposing that additional criterion.

A second possibility is that we were missing important model inputs. We began by discussing the need for many features and multiple treatments in treatment selection models. Although our model had a rich set of features, we may have been missing features that might have improved underlying model performance or are critical for selecting among these smoking cessation treatments. Our data come from a large comparative effectiveness trial conducted by a nationally recognized center and designed by foremost experts in the field [@bakerEffectsNicotinePatch2016]. The baseline assessment was quite comprehensive and was based on domain expertise and decades of research. Thus, it seems unlikely that we could be missing enough important self-report features to bridge the gap in our benefit of treatment selection.

<!--JJC:  you could acknowledge that the ML approach wouuld allow for many genes rather than single candidate but then still note that although this has promise, it will come at cost of accessibility  - at least today-->
One category of features we did not include was biological markers or genetic features, which have generated excitement for their potential for treatment selection [@chenPathwaysPrecisionMedicine2018]. However, extant literature has primarily used single candidate genes or biomarkers, has also not found long-term benefits, and has not translated well into real-world settings [@chenGeneticVariantCHRNA52020; @shahabDoesNicotineMetabolite2019; @schnollNicotineMetabolicRate2009; @glatardAssociationNicotineMetabolism2017; @chenowethNicotineMetaboliteRatio2016; @lermanUseNicotineMetabolite2015; @siegelUseNicotineMetabolite2020]. Additionally, any potential improvement from including biological or genetic features carries an associated cost to implementation given the relative inaccessibility of genetic and biological testing [@maceachernMachineLearningPrecision2021]. 

We may also have been missing model inputs on the treatment side. For example, bupropion is another first-line smoking cessation medication [@cahillPharmacologicalInterventionsSmoking2013]. It may be that some features in this model do not differentiate among C-NRT, nicotine patch, or varenicline but would differentiate between one of these treatments and bupropion. There may be individuals in our sample for whom none of our included treatments were optimal, but bupropion might have been. If true, including bupropion could have improved model performance *and* increased clinical benefit. 

<!-- JJC: these next paragraphs are important but I think there is opportunity to shorten this by at least a paragraph-->
A third, notable possibility is that our model did not capture dynamic changes over time. It may be that the same features predict treatment success across time. However, because these characteristics can change dynamically *within an individual*, what was the right treatment based on pre-quit characteristics is no longer the right treatment by 12 weeks, 6 months, or beyond. Many of the features used for prediction in this model were baseline measurements of *current* states - withdrawal, dependence, confidence/motivation to quit, time around other smokers, distress tolerance, depression symptoms, among others. Even features that feel more "static" like employment or marital status can be subject to change. 

Dynamic change is the rule rather than the exception when it comes to chronic diseases like substance use disorders. Tobacco and other substance use disorders are dynamic and relapsing; both risk for use and the factors driving that risk fluctuate over time [@brandonRelapseRelapsePrevention2007]. This change over time has been identified as a key barrier to overcome for precision mental health goals in addiction [@oliverPrecisionMedicineAddiction2017]. 

Accounting for dynamic change will require ongoing assessment of key features that predict treatment success. Such monitoring is now feasible given developments in personal sensing (i.e., in situ data collection via sensors embedded in individuals' daily lives) [@epsteinPredictionStressDrug2020; @soysterPooledPersonspecificMachine2022; @chihPredictiveModelingAddiction2014; @baeMobilePhoneSensors2018; @wyantMachineLearningModels2023] and is acceptable to individuals with substance use disorders [@wyantAcceptabilityPersonalSensing2023]. Sensing via ecological momentary assessment is well-positioned to capture the self-report items used as features in this treatment selection model. Although frequently answering a 50-item survey is likely not feasible, it may be that more proximal features have greater predictive power, thus requiring fewer features for successful treatment selection. Indeed, in previous work in our laboratory, we have been able to predict hour-by-hour probabilities of alcohol lapses quite accurately (auROCs > 0.9) using features derived from only 10 self-report items that were measured much closer to the outcome [@wyantMachineLearningModels2023].

Using dynamic monitoring, we can select and adapt treatments and supports over time. There are certainly opportunities to adapt medications – for example, titrating doses or identifying moments when someone could benefit most from an ad lib treatment like a nicotine lozenge. In general, medications are more static treatments that are less well-suited to dynamic changes, but they do not need to be used on their own. We observed that matching people to the right medication improves treatment success at 4 weeks. Medications could help create early treatment success that positions people to engage with additional supports that do adapt more dynamically. Alternative treatments and supports via web-based interventions and mobile health apps may offer platforms for sustainable, scalable, ongoing support. With ongoing monitoring, we could even recommend specific modules and individual tools that map onto currently important features affecting treatment success. Mapping risk factors to specific supports is likely quite complex and will require considerable future research. But if we hope to advance precision mental health for smoking, addiction, and even mental health broadly, we must consider the dynamic nature of risk and recovery inherent in these conditions.

### Interpreting our treatment selection model


Several recent reviews have noted that the low interpretability of "black box" machine learning models may impede their utility for clinical and public health goals [@maceachernMachineLearningPrecision2021; @mooneyBigDataPublic2018; @cohenTreatmentSelectionDepression2018]. Consequently, we aimed to make our model as interpretable as possible. We identified features that predict differential treatment success (i.e., features that interact with treatment to help us select among treatments). Our best selected model configuration retained 74 interaction terms spanning 52 unique items. Each feature's associated global Shapley value, which indicates overall magnitude of feature importance, was relatively small. This finding supports what has long been suspected in precision mental health: There is no one feature that explains sufficient variance to make differential predictions by treatment on its own. Rather, each feature offers only a small contribution, but many features together can guide treatment selection.

We also identified features that predict treatment success overall (i.e., "main effects"). These features contribute to the smoking cessation literature and support the conclusion from a recent review that predictors of treatment success span many categories [@bickelPredictorsSmokingCessation2023]. We found similar breadth in important features in this model: economic (e.g., income), environmental (e.g., living with another smoker), sociodemographic (e.g., marital status), psychological (e.g., depression diagnosis), physical health (e.g., pain interfering with daily activities), and smoking use/history (e.g., longest previous quit attempt) characteristics all contributed to predicting treatment success. 

These "main effect" features may yet help to advance precision mental health goals. These features may represent mechanisms underlying smoking cessation success and thus offer targeted areas for future treatment development. They also could be used to tailor existing treatments to increase success across individuals. Main effect features in our model might also interact with other treatments not included in this study such as bupropion. 

#### The role of demographic features

Several demographic features emerged as important interactive features for treatment selection: race, sex, income, and marital status. Ethnicity did not emerge as an important interactive feature, and race-based interactive features were specifically related to identifying as a Black or White individual. However, the limited representation of Hispanic, Latino/a, Asian, Multiracial, and Native American/Alaska Native individuals in this sample warrants caution in drawing conclusions about the (lack of) utility of ethnicity- and other race-based features for treatment selection. 

In some contexts, it would be problematic to use features that tap into constructs delineating marginalized identities such as race or socioeconomic status. For example, making decisions about who gets insurance (or doesn't) and who gets released earlier from incarceration (or doesn't) based on race is discriminatory (e.g., [@farayolaEthicsTrustworthinessAI2023]). However, in precision mental health, we are deciding not *who* should receive treatment but *which* treatment to give a specific patient. We can take advantage of experiential or symptomatological differences as a result of characteristics such as race, ethnicity, sex, income, or comorbid health conditions to improve treatment outcomes across vulnerable subpopulations.

#### The risk of transparency

<!-- maybe combine with the earlier section on interpretability-->
When patients and clinicians can easily see non-intuitive features, they may be dissuaded from using or trusting this treatment selection model. They may have similar concerns if they see demographic features are being used to make treatment decisions. With high-dimensional data and subsequent complex modeling, even the direction of an effect can be non-intuitive because a parameter estimate indicates a feature's effect while accounting for all other features in the model. Regardless of the reason, the reality is that seeing non-intuitive questions on a treatment selection assessment or unpacking precisely how a model is making its decision may make patients or clinicians doubt the trustworthiness of the model. Careful consideration will be required before implementation to assess acceptability of treatment selection models for all stakeholders. 

### Future directions


We made a concerted effort in this project to evaluate how our treatment selection model would perform for new patients using rigorous cross-validation procedures. Regardless, the ultimate test of this model's clinical benefit will be in a prospective trial. This trial will offer two tests. First, it will assess whether using this model is feasible and acceptable to patients and clinicians in clinical practice. Second, we can evaluate the benefit of our treatment selection model in an entirely new sample. Individuals who receive their model-predicted best treatment could be compared to any one of several possible comparison groups. Individuals in the comparison group could receive a random treatment assignment (mimicking clinical trials). Alternatively, they could receive clinician-assigned treatment to mirror traditional treatment selection (and best clinical practice). Another option is that we could compare to a simpler model. For example, there is evidence that treatment adherence is higher when people choose their preferred treatment [@cropseyPilotTrialVivo2017]. Thus, treatments in the comparison group could be assigned based on patient preference. Each comparison offers different advantages and disadvantages that should be considered thoughtfully when designing a prospective trial.

### Conclusion


Overall, this study has potential for immediate benefit to individuals looking to quit smoking. Our treatment selection model can improve the probability of abstinence during early recovery by a statistically significant and clinically meaningful margin. Moreover, we aimed to address health inequities by using a relatively low-burden assessment that uses widely accessible features and by ensuring our model provides equal if not relatively greater clinical benefit for individuals from several minoritized demographic groups. This treatment selection model may serve as an initial tool embedded in a continuing care landscape where treatments adapt dynamically over time. The ultimate test of our model will be in a prospective trial that assesses its feasibility, acceptability, and effectiveness in clinical practice. We are optimistic about the promise our model holds to improve the public health burden of cigarette smoking.
