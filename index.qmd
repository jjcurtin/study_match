---
title: Machine learning-assisted treatment selection for smoking cessation
author:
  - name: Gaylen E Fronk
    orcid: 0000-0001-6653-9699
    corresponding: false
    roles: []
    affiliations:
      - Department of Psychology, University of Wisconsin-Madison
  - name: John J. Curtin 
    orcid: 0000-0002-3286-938X
    corresponding: true
    email: jjcurtin@wisc.edu
    roles: []
    affiliations:
      - Department of Psychology, University of Wisconsin-Madison 
keywords:
  - Substance use disorders  
  - Precision mental health  
  - Cigarette smoking  
  - Machine learning  
  - Treatment selection  
abstract: |
  This study found some pretty cool results that have both high impact and important clinical implications.  For example ...
plain-language-summary: |
  We can select among treatments!!! Without a lot of questions!!! And with the opportunity to assess remotely for greater access!!! AND IT MAKES A BIG DIFFERENCE!!!
key-points:
  - Take away point 1 
  - Take away point 2
date: last-modified
bibliography: references.bib
bibliographystyle: apa 
number-sections: false 
editor_options: 
  chunk_output_type: console
---

## Introduction

There is a critical flaw in how we traditionally select treatments for patients. Selection primarily relies on population-level effectiveness; however, what works best at a population level does not necessarily work best for a given patient. For example, although treatment A may be more effective than treatment B *on average*, it may be that treatment B is markedly more effective for a *specific* patient. Moreover, when our treatments are not as effective as they need to be, the pipeline for developing new treatments to fill those gaps is slow and costly: on average, new drug development takes 9-12 years and costs hundreds of millions of US dollars [@dicksonCostNewDrug2009]. 
    
Precision medicine may offer a solution. Precision medicine, sometimes called personalized medicine, seeks to guide treatment selection using individual difference characteristics that are likely to predict treatment success for each patient [@bickmanImprovingMentalHealth2020]. Successful precision medicine would increase the likelihood of treatment success for each patient because each patient receives the treatment predicted to work best for them. It would also improve treatment effectiveness rates across the population because each treatment is administered only to the patients for whom that treatment is expected to be their best option [@bickmanAchievingPrecisionMental2016; @bickmanImprovingMentalHealth2020]. Precision medicine may also be more resource-efficient. Existing treatments can be optimized by directing them to the *right* patients - positioning this approach for immediate impact to patients. We can also use precision medicine to guide treatment development more strategically as we identify factors that promote treatment success or sub-populations for whom no existing treatments are optimal. This ensures that the time and cost that go into new treatment development are reserved for specific niches of need.

Researchers have pursued precision medicine for decades. Emphasis on personalizing treatments has been tied closely to genetic factors and thus has grown rapidly with the ascendancy of advanced genetic methods such as genome-wide association studies, polygenic scores, and functional annotation [@wrayResearchReviewPolygenic2014; @bogdanPolygenicRiskScores2018; @kranzlerPrecisionMedicinePharmacogenetics2017; @huLeveragingFunctionalAnnotations2017; @finucanePartitioningHeritabilityFunctional2015]. Perhaps the most meaningful progress towards precision treatments has been made in the cancer domain; for example, some chemotherapy drugs that are not effective at a population level have been shown to help individuals with specific non-small-cell lung carcinoma tumor mutations [@rosellErlotinibStandardChemotherapy2012].

Precision mental health is the application of the precision medicine paradigm to mental health conditions [@derubeisHistoryCurrentStatus2019; @inselNIMHResearchDomain2014; @bickmanAchievingPrecisionMental2016]. There is a critical need for improved mental health treatments. In 2019, 51 million Americans had an active mental illness, and 20 million US adults had an active substance use disorder [CITE from john's tedx]. Mental health disorders are leading causes of disability and death [@whitefordGlobalBurdenDisease2013; @centersfordiseasecontrolandpreventioncdcAnnualAverageUnited]. Indeed, psychiatric disorders account for enormous economic burden; some estimates suggest higher even than chronic medical disorders like cancer [@trautmannEconomicCostsMental2016; @substanceabuseandmentalhealthservicesadministrationusFacingAddictionAmerica2016]. Additionally, mental healthcare is plagued by disparities related to race, ethnicity, geographic region, and socioeconomic status: vulnerable sub-populations are more likely to have higher rates of mental health disorders and more difficulty accessing treatment [@moralesCallActionAddress2020; @barksdaleInnovativeDirectionsAdvance2022; @officeofthesurgeongeneralusMentalHealthCulture2001; @jacobsonDigitalTherapeuticsMental2022]. Treatments for mental health conditions are also usually no more than moderately effective, and many treatments for the same disorder can be quite comparable, making it difficult to select among them. For example, prolonged exposure therapy and cognitive processing therapy are two gold-standard treatments for post-traumatic stress disorder, and comparative trials have shown similar effectiveness [@lewisPsychologicalTherapiesPosttraumatic2020]. Similarly, a recent review of anxiety treatments concluded that treatment effectiveness has not changed over decades despite the development of many new treatments in that period [@weiszArePsychotherapiesYoung2019]. 

Like precision medicine, many researchers have pursued precision mental health. An early research example comes from the substance use disorder domain: the Project MATCH Research Group attempted to match people with alcohol use disorder to a particular treatment based on individual differences [@projectmatchresearchgroupProjectMATCHMatching1993; @projectmatchresearchgroupMatchingAlcoholismTreatments1998]. They assigned people to one of three treatments and evaluated whether any of ten patient attributes (e.g., gender, social support, symptom severity) interacted with treatment effectiveness. Despite a large body of evidence supporting the importance of characteristics such as these for predicting alcohol-related outcomes, they did not find that any interactions had an impact on drinking. Although Project MATCH did not succeed in its goal of matching patients to treatments, it set the stage for future precision mental health research. Many researchers have followed in their footsteps as the understanding has grown that neither mental health diagnoses nor treatments are one-size-fits-all [@derubeisHistoryCurrentStatus2019]. This research has primarily focused on selecting among treatments for depression (e.g., [@derubeisPersonalizedAdvantageIndex2014; @webbPersonalizedPredictionAntidepressant2019]; for review, see [@cohenTreatmentSelectionDepression2018]).
    
Despite the abundance of research, much less progress has been made in precision mental health than in precision medicine [@bickmanAchievingPrecisionMental2016; @bickmanImprovingMentalHealth2020; @kranzlerPrecisionMedicinePharmacogenetics2017; @oliverPrecisionMedicineAddiction2017; @kesslerPragmaticPrecisionPsychiatry2021]. There may be several reasons for this. First, many factors influence heterogeneous, complex clinical phenomena like mental health diagnoses and treatment success [@feczkoMethodsChallengesAssessing2020]. Thus, any single feature (i.e., predictor variable) cannot account for more than a small portion of the variance in treatment success [@kesslerPragmaticPrecisionPsychiatry2021; @inselNIMHResearchDomain2014]. This idea is comparable to the shift in understanding within genetics: research has moved away from candidate gene studies to polygenic approaches that rely on small contributions from many genes [@bogdanPolygenicRiskScores2018; @chenPathwaysPrecisionMedicine2018; @wrayResearchReviewPolygenic2014]. Second, mental health disorders generally lack the "rigorously tested, reproducible, clinically actionable biomarkers" that have paved the way for progress in precision medicine [@inselNIMHResearchDomain2014; @kranzlerPrecisionMedicinePharmacogenetics2017]. This means that we must look to other categories of features such as demographic characteristics, psychological traits, and environmental variables [@bickmanImprovingMentalHealth2020].

Precision mental health efforts thus far, however, have largely focused on tailoring treatments by identifying a single factor that divides individuals within a diagnostic category into subgroups that can be treated differently [@derubeisHistoryCurrentStatus2019]. Given the known heterogeneity in mental health diagnoses, groups differentiated on a single factor are not homogeneous - group-level tailoring may not be sufficiently granular for mental health disorders. Thus, it is perhaps unsurprising that models that consider only one or a small handful of features - which also limits considering concurrently features across categories - have failed to capture the real-world complexity underlying these complex clinical phenomena.

Additionally, because precision mental health models are typically developed and evaluated in the same sample, the models may become very overfit to that sample [@jonathanUseCrossvalidationAssess2000]. This problem is exacerbated in precision mental health because sample sizes in psychological research have remained quite small despite recommendations to increase sample size [@marszalekSampleSizePsychological2011]. Consequently, precision mental health models do not generalize well to new patients. Clinical implementation of these models requires that they provide accurate, patient-level recommendations about treatment selection for *new* patients.

These pitfalls interact with each other. To capture sufficient complexity to predict treatment success, we need to increase the total number of features in precision mental health models. Incorporating more features, however, makes overfitting the data more likely. Thus, successful precision mental health requires an analytic approach that can handle high-dimensional data without becoming too overfit to generalize to new patients.

### Applying machine learning approaches

Applying machine learning may be able to address these limitations of traditional analytic techniques to advance precision mental health goals [@bickmanAchievingPrecisionMental2016; @dwyerMachineLearningApproaches2018; @maceachernMachineLearningPrecision2021; @mooneyBigDataPublic2018]. Machine learning is an alternative analytic technique that uses statistical algorithms trained on high-dimensional arrays (hundreds or even thousands) of features [@jamesIntroductionStatisticalLearning2013; @kuhnAppliedPredictiveModeling2018; @ngMachineLearningYearning2018]. Flexibly considering many features simultaneously means these models can tap the tangled web of constructs that comprise complex clinical phenomena. Critically, this allows researchers to consider many features in the same model – unlike previous precision mental health research that was limited to considering very few features simultaneously [@maceachernMachineLearningPrecision2021]. This high dimensionality across and within sets of related features is necessary to explain a high portion of variance in person-level treatment success. 

Although machine learning models can handle very large numbers of features, this capacity comes at a cost, referred to as the “bias-variance trade-off” [@jamesIntroductionStatisticalLearning2013; @ngMachineLearningYearning2018]. Too many features (particularly correlated features) yield unstable models that vary strongly based on the data used to develop them. High variance compromises model generalizability because a high variance (e.g., very flexible) model may not predict very accurately in new data. However, too few features (as well as other constraints on model characteristics) yield biased models that also do not predict well because they miss important predictive patterns and relationships. Machine learning uses various techniques (e.g., regularization, hyperparameter tuning, simultaneous consideration of many model configurations) to optimize this bias-variance trade-off to accommodate high-dimensional sets of features while reducing overfitting [@jamesIntroductionStatisticalLearning2013; @kuhnAppliedPredictiveModeling2018; @ngMachineLearningYearning2018; @mooneyBigDataPublic2018]. Thus, machine learning methods may allow us to build precision mental health models that both capture clinical complexity and generalize accurately to new data.

In addition to affecting the bias-variance trade-off, high-dimensional datasets and complex modeling procedures can make interpretation difficult in machine learning [@maceachernMachineLearningPrecision2021; @mooneyBigDataPublic2018; @cohenTreatmentSelectionDepression2018]. Fortunately, advances in interpretable machine learning (e.g., SHAP method for feature importance [@lundbergUnifiedApproachInterpreting2017]) can help to counteract this concern. These techniques allow us to consider many features across categories while identifying which features contribute most to model performance.

Finally, machine learning provides rigorous resampling techniques (e.g., cross-validation) to fit and evaluate models in separate data [@jamesIntroductionStatisticalLearning2013]. Consequently, models generalize well to new patients because they are evaluated on out-of-sample prediction. In a simplest case, data can be divided into held-in and held-out samples. More sophisticated resampling techniques such as cross-validation involve dividing the data many times to create multiple held-in and held-out samples [@krstajicCrossvalidationPitfallsWhen2014; @jonathanUseCrossvalidationAssess2000]. These approaches offer significant advantages for 1) accurately selecting a best model among multiple model configurations, and 2) estimating how well that model will perform when applied to new data (e.g., new patients in a clinical setting). Applying machine learning can accomplish the goal in precision mental health of accurate, robust treatment selection for new patients.

### Cigarette smoking as a critical precision mental health target

#### Public health importance of cigarette smoking

Cigarette smoking could benefit greatly from combining precision mental health and machine learning. Smoking remains an enormous public health burden. Tobacco is the number one cause of preventable death in the U.S. and accounts for more than 480,000 deaths annually [@nationalcenterforchronicdiseasepreventionandhealthpromotionusofficeonsmokingandhealthHealthConsequencesSmoking2014; @schlamInterventionsTobaccoSmoking2013; @corneliusTobaccoProductUse2020]. Compared to people who have never smoked, individuals who smoke have two- to three-fold likelihood of death across causes and lose over a decade of life expectancy [@jhaprabhat21stCenturyHazardsSmoking2013]. 

Although rates of smoking have declined considerably, approximately 14% of U.S. adults continue to smoke daily or near-daily [@corneliusTobaccoProductUse2020]. Additionally, cigarette smoking rates remain much higher in potentially vulnerable populations. This includes people with chronic or severe mental illness [@bakerSmokingTreatmentReport2021]; Native American and non-Hispanic Black individuals [@jamalCurrentCigaretteSmoking2015a; @corneliusTobaccoProductUse2020]; individuals who are economically and educationally disadvantaged [@jamalCurrentCigaretteSmoking2015a; @corneliusTobaccoProductUse2020]; people with other substance use disorders [@kellyPrevalenceSmokingOther2012]; people in the criminal justice system [@cropseySmokingFemalePrisoners2004; @harrisonCigaretteSmokingMental2020]; people experiencing homelessness [@baggettTobaccoUseHomeless2013; @soarSmokingAmongstAdults2020]; people who are insured through Medicaid or uninsured [@jamalCurrentCigaretteSmoking2015a; @corneliusTobaccoProductUse2020]; and individuals who identify as lesbian, gay, or bisexual [@jamalCurrentCigaretteSmoking2015a; @corneliusTobaccoProductUse2020].

#### Smoking cessation treatment

Despite the severity of the problem, treatment has had relatively limited reach [@bakerSmokingTreatmentReport2021]. A survey of almost 16000 US adults who use cigarettes showed that the most commonly used strategies by far to quit were giving up cigarettes all at once and gradually cutting back, with a much smaller proportion using evidence-based treatments [@caraballoQuitMethodsUsed2017]. Best estimates suggest it takes about 30 attempts to quit successfully; given that as many as 40-50% of former smokers quiit on their first serious attempt, there are many individuals for whom it takes abundantly more attempts [@chaitonEstimatingNumberQuit2016]. Individuals are less likely to achieve sustained abstinence when they have one or more failed quit attempts within the past year [@partosQuittingRollercoasterHow2013], highlighting the need for individuals to receive effective treatment as early as possible.

For those who do get treatment, the best available smoking cessation treatments are modestly effective. The medications varenicline and combination nicotine replacement therapy (C-NRT) are consistently identified as the most effective options when combined with psychosocial counseling [@cahillPharmacologicalInterventionsSmoking2013; @schlamInterventionsTobaccoSmoking2013; @bakerSmokingTreatmentReport2021; @rigottiTreatmentTobaccoSmoking2022]. Guidelines recommend that clinicians consider either of these two medications first given their established efficacy [@fioreClinicalPracticeGuideline2008]. These medications appear to be equally (though modestly) effective: a meta-analysis demonstrated comparable effectiveness rates [@cahillPharmacologicalInterventionsSmoking2013], and the first randomized controlled trial (RCT) directly comparing varenicline and C-NRT did not find a difference between them [@bakerEffectsNicotinePatch2016]. 

Typically, 6-month abstinence rates hover around 30-35% for these best smoking cessation medications combined with psychosocial counseling [@cahillPharmacologicalInterventionsSmoking2013; @fioreClinicalPracticeGuideline2008]. Treatment with an FDA-approved medication doubles the likelihood that an individual will quit successfully [@bakerSmokingTreatmentReport2021]. These rates represent a best-case scenario in that clinical trial data involve treatment regimens that are rigorously followed and optimized for adherence. Additionally, because several first-line (i.e., FDA-approved) smoking cessation treatments have comparable population-level effectiveness rates, population effectiveness alone cannot guide selection among smoking cessation treatments. Indeed, national clinical guidelines note that “there are no well-accepted algorithms to guide optimal selection” between any of the first-line medications ([@fioreClinicalPracticeGuideline2008], p. 44). These facts suggest a critical need for precision mental health approaches in the cigarette smoking domain.

#### Previous precision mental health & machine learning research

In recognition of the need for improved treatment effectiveness, and the potential for personalized treatment approaches to provide that improvement, many smoking cessation researchers have pursued precision mental health research in recent years. 

There is a reasonable body of evidence identifying "prognostic" factors [@cohenTreatmentSelectionDepression2018] related to smoking cessation. This research aims to predict who will or will not be able to quit successfully [@laiDevelopmentMachineLearning2021; @kaufmannRateNicotineMetabolism2015; @kayeSearchingPersonalizedMedicine2020a; @piperPrecisionSmokingCessation2017a; @issabakhshMachineLearningApplication2023; @etterPredictingSmokingCessation2023]. Similarly, there is research using machine learning that classifies individuals who smoke (vs. individuals who do not; [@pariyadathMachineLearningClassification2014a]). A systematic review identified that predictors of smoking cessation outcomes span many categories: economic variables, environmental variables, sociodemographic variables, engagement in treatment, physical health variables, psychological variables, neurocognitive factors, biomarkers, and factors related to smoking history and severity [@bickelPredictorsSmokingCessation2023].

Related research in this area seeks to understand who will succeed using a single treatment. Massago and colleagues (2024) examined medical records of individuals completing cognitive-behavioral treatment (CBT) and built a machine learning model to predict who is most likely to quit smoking using CBT. They note that this model "can be used to establish priorities when the demand is higher than the capacity" ([@massagoApplicabilityMachineLearning2024], p. 10). Other research has used tree-based machine learning models to show that delay discounting tasks can differentiate individuals who respond to group CBT from individuals who do not [@coughlinMachineLearningApproachPredicting2020a]. 

This research investigating prognostic factors is critical for understanding mechanisms that underlie smoking cessation success. Progress in this area may allow us to identify processes that improve the likelihood of quitting successfully, and we may be able to intervene in some of these mechanisms prior to a quit attempt or improve design of preventative interventions. For example, individuals are less likely to succeed in a quit attempt when cigarettes are available and when confidence is low [@bickelPredictorsSmokingCessation2023]. Unsurprisingly, key steps in pre-cessation and cessation counseling include getting rid of cigarettes and increasing confidence to quit. As more prognostic factors are identified, we can improve overall treatment effectiveness by targeting key predictors of success. 

Although models that identify prognostic factors are important, they do not offer an actionable way forward to select among treatment options. Even research that informs us as to who might succeed within a specific treatment has limited utility for treatment selection: what do we do for a patient who is predicted not to succeed using that treatment? If different single-treatment prognostic models offer conflicting recommendations, how should a clinician or patient proceed? To *select among* treatments, we need "prescriptive" predictors [@cohenTreatmentSelectionDepression2018], or predictors that determine which treatment to prescribe. This type of predictor is what allows us to select the best treatment for a given patient. 

Some research has begun to investigate factors that can allow selection among treatment options, particularly genetic factors and biomarkers (for review, see [@chenPathwaysPrecisionMedicine2018]). Chen and colleagues found that variants in the cholinergic receptor nicotinic alpha 5 subunit (CHRNA5) predict differential treatment success among Black individuals who smoke: C-NRT was more effective for people with certain genotypes, whereas varenicline was more effective for people with other genotypes [@chenGeneticVariantCHRNA52020]. Many researchers have investigated the role of the nicotine metabolite ratio (NMR), a phenotypic biomarker for nicotine metabolism that is easier to test than genotypes. Overall, it appears that slow metabolizers report greater cessation success with NRT medications, whereas varenicline is more effective for fast metabolizers [@shahabDoesNicotineMetabolite2019; @schnollNicotineMetabolicRate2009; @glatardAssociationNicotineMetabolism2017; @chenowethNicotineMetaboliteRatio2016; @lermanUseNicotineMetabolite2015], though a review by Siegel and colleagues notes that "real-world evaluations of NMR to personalize treatment for smoking cessation have produced mixed findings so far" ([@siegelUseNicotineMetabolite2020], p. 265).

This research has exciting implications for selecting among treatments. However, using biologic and genetic factors comes with downsides when considering accessibility and implementation. Though the necessary technology is improving, the testing required to collect these data is neither widely available nor accessible [@maceachernMachineLearningPrecision2021]. Access to treatment is a known barrier in mental healthcare and a contributing factor driving mental healthcare disparities;  [@jacobsonDigitalTherapeuticsMental2022]. Using genetic and biological factors is likely to favor privileged individuals who have insurance that covers specialized care and specialty testing. Some have noted that biomarker testing for NMR is more affordable than genetic testing; however, there remain methodological challenges associated with establishing cut-off points for slow vs. fast metabolizers, particularly among vulnerable sub-populations for whom there are no NMR data available yet who experience greater smoking rates or associated risks (e.g., Native American/American Indian people, pregnant women, people who drink heavily or have other substance use disorders, people with serious mental illness, and individuals who identify as LGBTQ; [@siegelUseNicotineMetabolite2020]). In contrast to biological or genetic data, other individual difference features may be better positioned for implementation. For example, many features could be measured easily via self-report, which may enable remote assessment for individuals without insurance or access to in-person medical care.

Other research has investigated these types of non-biological factors as potential moderators of treatment effectiveness. Kaye and colleagues (2020) examined whether varenicline might be more effective than C-NRT among individuals who smoke and binge drink; however, varenicline's effects did not vary as a function of drinking status [@kayeSearchingPersonalizedMedicine2020a]. Piper and colleagues conducted a series of studies with the goal of advancing precision mental health for smoking cessation. They used a factorial design to identify ideal treatment combinations at the group level (e.g., pre-quit medication, pre-quit counseling, counseling modality, cessation medications; [@piperIdentifyingEffectiveIntervention2016]). They followed up by exploring moderators of intervention main effects and interactions and found that psychiatric history moderated some treatment effects [@piperPrecisionSmokingCessation2017a]. They also looked at whether various treatment components affected their purported treatment mechanisms to help elucidate how these treatments may be producing their effects [@piperPrecisionSmokingCessation2017]. These studies took advantage of largely self-report data, making any resulting models more clinically implementable. However, each study examined only a single moderator or examined each moderator in a separate model. These analyses were ideally suited for clearly identifying moderators and explaining mechanisms, but they were less well-suited for finding high-dimensional sets of features that can predict differential treatment responses.

#### Opportunities for differential treatment selection

These studies of biological variables and single moderators suggest there is potential that some treatments may work better than others for a specific individual. Prognostic factors identified in other research may also operate as prescriptive factors - they just have not necessarily been tested that. WEAVE THROUGHOUT

Although there is not yet much research selecting among treatments, there may be reason to expect that some treatments may work better than others for a specific individual. 

First, there is enormous heterogeneity among people who smoke cigarettes. Individuals may differ with respect to the etiology of their tobacco use disorder, the severity of their dependence and/or withdrawal symptoms, historical factors related to their tobacco use (e.g., age of first use, years smoking, number of previous quit attempts), and barriers to initiation and/or retention in smoking cessation treatments [@oliverPrecisionMedicineAddiction2017; @wangSociodemographicVariabilityAdolescent2009; @zhengIdiographicExaminationDaytoDay2013]. These factors that affect the development and course of their disorder could include demographic traits, personal medical history, and many other key individual difference characteristics. Although this heterogeneity is typically neglected when selecting among available treatments, precision mental health approaches would instead allow us to take advantage of it. 

Second, smoking cessation medications have distinct pharmacological mechanisms of action at nicotinic acetylcholine receptors (nAChRs), which may affect how helpful they are for different people. Nicotine replacement therapy (NRT) provides nicotine, a full agonist at nAChRs. Different NRTs provide nicotine differently. C-NRT consists of a nicotine patch and ad libitum nicotine lozenge use. The patch offers transdermal administration of a low, steady dose of nicotine. Some people who smoke cigarettes may rely on this low, steady nicotine level to replace nicotine from cigarettes. Lozenges provide oral administration of nicotine with more rapid onset, which could help individuals who need a quick boost during craving. 

Other individuals who smoke may benefit from a medication like varenicline. In contrast to NRTs, varenicline is a partial agonist at nAChRs [@cahillNicotineReceptorPartial2016]. Partial agonists have a pharmacological action that is somewhere between full agonists and antagonists, depending on the level of surrounding neurotransmitter. In the absence of a full agonist or endogenous neurotransmitter, partial agonists can act as a functional agonist with lower activity than a full agonist. In the presence of a full agonist (e.g., a cigarette) or endogenous neurotransmitter, however, they act as functional antagonists because their binding to the receptor limits the amount of binding from the full agonist and consequently reduces that response [@jordanDiscoveryDevelopmentVarenicline2018; @liebermanDopaminePartialAgonists2004]. 

Thus, varenicline may be more pharmacologically flexible than NRT medications: when an individual is not smoking, it can produce milder, nicotine-like effects; if an individual begins smoking again, it could block or reduce full agonist (cigarette nicotine) activity at the receptor. This would be expected to reduce the pharmacological effect of nicotine, likely reducing the behavioral pleasure of smoking [@cahillNicotineReceptorPartial2016]. Although C-NRT has some behavioral flexibility built in (i.e., combination of slow, steady dosing with faster-acting lozenges that can be used in response to internal states or environmental cues), it acts exclusively as a full agonist at nAChRs and cannot exert antagonist-like actions. 

Third, features across several behavioral or environmental domains may also guide treatment selection, alone or in combination with medication mechanisms of action. Research identifying prognostic factors described in the previous section has shown there are *many* factors that predict smoking cessation, and these factors span *many* clinical and behavioral domains [@bickelPredictorsSmokingCessation2023]. It is possible that some of these prognostic factors may act as prescriptive factors as well (i.e., be able to help prescribe/select a treatment), but this has not yet been tested. Moreover, these many factors have not been considered simultaneously in a model as may be needed to unpack complex clinical phenomena like differential treatment response.

For example, some cigarette smokers may have strong cravings with good self-monitoring. These characteristics may make treatments such as nicotine lozenges or gum more effective because they are aware enough of their own craving to get a quick “hit” of nicotine when needed. In contrast, an individual with strong craving *without* good self-monitoring may be less likely to succeed with C-NRT because they cannot identify their moments of need for lozenges or gum. For these individuals, C-NRT may not offer benefit over a single NRT (e.g., nicotine patch alone) despite its higher effectiveness in the broader population [@cahillPharmacologicalInterventionsSmoking2013]; instead this multi-component treatment may feel overwhelming, reducing adherence and thus ultimate effectiveness. Some smokers may be prone to side effects from a specific treatment, causing them to discontinue treatment prematurely, though they may not have had the same adverse reactions to a different treatment. Environmentally, an individual who lives with other smokers may benefit from a partial agonist treatment like varenicline because any secondhand smoke would produce less effect. 

Any of these characteristics, among others, could powerfully inform treatment selection for cigarette smoking cessation. These examples illustrate the potential clinical benefit of using a precision mental health paradigm to guide treatment selection for smoking cessation. They also point to the value of analytic techniques that can incorporate many features simultaneously and consider complex interactions among features. Although these examples were selected because they are more intuitive, there are likely other unexpected ways that treatment success differs across people. Because machine learning models are built bottom-up based on patterns in the data, rather than top-down based on established theories or clinical intuition, these models may reveal unanticipated features that could meaningfully guide treatment selection for cigarette smoking cessation.

Finally, there is some evidence that individuals respond differently to different treatments. One large study that examined multiple medication-assisted quit attempts found that individuals who switched medications were more likely to quit than individuals who used the same medication again or who did not use a medication on the first quit attempt but added one at the second [@heckmanEffectivenessSwitchingSmokingCessation2017]. Relatedly, there is some evidence that re-treatment with the same medication as a previous, unsuccessful quit attempt is not effective [@fioreClinicalPracticeGuideline2008; @tonnesenRecyclingNicotinePatches1993] Indeed, Gonzales and colleagues found that “abstinence rates are more than threefold lower for NRTs” during re-treatment compared to initial treatment using the same medication ([@gonzalesRetreatmentVareniclineSmoking2014], p. 391). A pilot trial showed that treatment adherence improves when individuals are given the opportunity to sample various NRT medications pre-quit [@cropseyPilotTrialVivo2017], and meta-analyses show only smokers who are highly dependent may benefit from 4 mg (vs. 2 mg) nicotine gum [@lindsonDifferentDosesDurations2019]. These data suggest differential preferences and even differential effectiveness despite a shared pharmacological mechanism of action across NRT medications. The research previously described using the biomarker NMR also supports differential treatment effectiveness [@siegelUseNicotineMetabolite2020]. Additionally, clinical research related to other psychological and psychiatric disorders has demonstrated differential treatment benefit on an individual basis (e.g., antipsychotic medications for schizophrenia [@roussidisReasonsClinicalOutcomes2013]; psychosocial interventions for depression [@cohenTreatmentSelectionDepression2018]), suggesting it is worth investigating whether the same is true in smoking cessation. 

## Specific Aims

The goal of this project was to produce a model that can serve as a decision-making tool to select among medication treatments for smoking cessation. Specifically, we pursued the following aims:

**AIM 1: Build a machine learning model to predict cigarette smoking cessation.** We built a model that could predict smoking cessation treatment success (i.e., point-prevalence abstinence) at 4 weeks post-quit. This model used retrospective data from a previously completed comparative effectiveness trial wherein individuals who smoke were randomized to treatments, richly characterized at baseline, and followed to assess short- and long-term abstinence [@bakerEffectsNicotinePatch2016]. 

**AIM 2: Evaluate the clinical benefit of using our prediction model for differential treatment selection.** We used the prediction model to identify the best treatment for each person and evaluate whether individuals who received their best treatment were more likely to be abstinent in the original trial at 4 weeks, 12 weeks (end-of-treatment), and 6 months post-quit. 

Across both Aims, we used rigorous resampling techniques to ensure that we evaluated our model's capacity to predict smoking cessation outcomes and select treatments in *new* patients. Additionally, we incorporated easy-to-collect self-report data as model inputs, and we employed a statistical algorithm that can reduce assessment requirements. These choices position our model for accessible clinical implementation.

## Methods

### Transparency & openness

<!-- NTS: already correct OSF page -->

We adhere to research transparency principles that are crucial for robust and replicable science. We reported how we determined the sample size, all data exclusions, all manipulations, and all study measures. We provide a transparency report in the supplement. Finally, our data, analysis scripts, annotated results, questionnaires, and other study materials are publicly available ([https://osf.io/qad4n/](https://osf.io/qad4n/)). 

We preregistered our analyses to evaluate clinical benefit that relied on significance testing. The preregistration can be found on our OSF page ([https://osf.io/qad4n/](https://osf.io/qad4n/)). These analyses also closely followed published research evaluating treatment selection models [@derubeisPersonalizedAdvantageIndex2014]. For our Bayesian hierarchical generalized linear models, we followed guidelines from the tidymodels team [@kuhnTidyposteriorBayesianAnalysis2022] that we have followed in other published research from our laboratory [@wyantMachineLearningModels2023]. For all other analyses, we restricted many researcher degrees of freedom via cross-validation. Cross-validation inherently includes replication: models are fit on held-in training sets, decisions are made in held-out validation sets, and final performance is evaluated on held-out test sets.

<!-- add TRIPOD reporting here if we do it - probably not for dissertation but maybe for published paper -->

### Data

The data for this project came from a completed randomized controlled trial conducted by the University of Wisconsin (UW) Center for Tobacco Research and Intervention (CTRI) [@bakerEffectsNicotinePatch2016]. This trial compared the effectiveness of three cigarette smoking cessation treatments (varenicline, combination nicotine replacement therapy [C-NRT], and nicotine patch). Briefly, 1086 daily cigarette smokers were enrolled in Madison, WI, USA and Milwaukee, WI, USA. Exclusion criteria included contraindicated medical (e.g., severe hypertension) or psychiatric conditions (e.g., severe and persistent mental illness), current use of contraindicated medications, and pregnancy or unwillingness to use appropriate methods of contraception while taking a study medication. Participants set a quit date with study staff and were enrolled for several weeks prior to the target quit date through at least 6 months following quitting smoking. 

#### Treatment conditions

Participants were randomly assigned to one of three medication conditions: varenicline, C-NRT, or nicotine patch. Each medication treatment lasted 12 weeks. For varenicline, participants began medication use prior to their quit attempt, starting with 0.5 mg once daily for 3 days, followed by 0.5 mg twice daily for 4 days, and 1 mg twice daily for 3 days. They continued use of 1 mg twice daily for 11 weeks following their quit date except in response to adverse effects. For C-NRT or nicotine patch, participants began using the patch on their quit date, starting with 21 mg for 8 weeks, followed by 14 mg for 2 weeks, and 7 mg for 2 weeks. All individuals who received C-NRT were also instructed to use 5 lozenges per day (2 or 4 mg nicotine lozenges determined by time to first daily cigarette) for the full 12 weeks except in the case of adverse effects.

All participants also received 6 sessions of motivational and skill-training counseling per clinical guidelines [@fioreClinicalPracticeGuideline2008]. 

#### Individual difference characteristics

Participants were comprehensively assessed for individual differences characteristics prior to treatment randomization. These characteristics fall into several domains expected to relate to cigarette smoking cessation: tobacco-related (e.g., cigarettes per day), psychological (e.g., psychiatric diagnoses, distress tolerance), physical health (e.g., vital signs), social/environmental (e.g., living with another person who smokes), and demographic (e.g., age, sex). A detailed list of all available individual differences variables appears in Table 1.

<!-- {{< embed notebooks/mak_tables.qmd#tbl-features >}} figure out why this isn't working -->

<!-- add table caption that includes citations for major measures per KW, susan is gathering references -->

#### Abstinence outcome

Throughout study participation, participants were assessed for biologically confirmed, 7-day point-prevalence abstinence (i.e., any smoking in the previous 7 days). Participants self-reported whether they had smoked over the past 7 days, and their report was biologically confirmed via exhaled carbon monoxide (CO). Participants were labeled as "abstinent" if their CO level was less than 6 parts per million (ppm; [@bakerEffectsNicotinePatch2016]). If participants self-reported any smoking in the past 7 days, their CO level contradicted their self-report (i.e., CO level > 6 ppm), or biological confirmation could not be confirmed, participants were labeled as "smoking." 

Our *primary prediction outcome* for our models was point-prevalence abstinence at 4 weeks post-quit. Later assessments of point-prevalence abstinence (12 weeks [end-of-treatment], 6 months) were used for clinical benefit analyses (see below) and supplemental analyses.

### AIM 1 analytic strategy

The steps below describe how we built our classification models to predict 4-week point-prevalence abstinence. We also built models predicting 26-week (6 month) point-prevalence abstinence for supplemental analyses; all model fitting and evaluation procedures were identical. Methods and results related to the 26-week (6 month) models appear in the Supplement.

#### Model configurations

Machine learning seeks to optimize the bias-variance trade-off: too many features can yield high-variance models that do not generalize well to new data; too few features can yield biased models that miss important predictive relationships. It is difficult to determine *a priori* where along the bias-variance continuum we need to be to maximize performance. Consequently, we considered many model configurations that differed across characteristics expected to affect bias and variance. These characteristics include feature engineering steps, dimensionality reduction approaches, different size feature sets, and hyperparameter values. The model configuration that performs best in new data is then identified as the approach that optimizes the bias-variance trade-off.

##### Statistical algorithm

We used the statistical algorithm Elastic Net Logistic Regression (GLMNet). This algorithm aligns with our primary goal of building a treatment selection model in several ways. 

First, it allows for explicit inclusion of interaction terms (i.e., including interactions between treatment and all other variables). This permits capturing many interactions that each account for a small portion of variance; as discussed, this seems critical to capturing the complexity of clinical phenomena. Second, GLMNet performs a degree of dimensionality reduction because it penalizes model complexity; thus, a model using this algorithm may require assessing fewer features than are initially considered in the model. This characteristic aligns with our intention to implement this model in clinical practice, where highly burdensome assessments are impractical and thereby not feasible. Finally, linear models such as GLMNet are often more interpretable and transparent because they produce parameter estimates for the features in the model. Several recent reviews have noted interpretability as a potential barrier to using machine learning approaches for clinical and public health goals; consequently, we aimed to prioritize interpretability [@maceachernMachineLearningPrecision2021; @mooneyBigDataPublic2018; @cohenTreatmentSelectionDepression2018].

Initial testing showed that GLMNet outperformed or performed comparably to models fit using several other well-established statistical algorithms (XGBoost, Random Forest). Thus, we had no reason not to prefer an algorithm that aligned well with our ultimate clinical goals.

The statistical algorithm GLMNet uses two hyperparameters (i.e., model tuning parameters): alpha and lambda. Candidate model configurations differed across sensible values of these hyperparameters that affect parameter estimates and dimensionality and thus affect the bias-variance trade-off.

##### Feature engineering

Feature engineering is the process of converting raw predictors into meaningful numeric and/or categorical representations (features) that improve model effectiveness [@kuhnFeatureEngineeringSelection2019]. A sample feature engineering script (i.e., tidymodels recipe) containing all feature engineering steps is available on our OSF study page ([https://osf.io/qad4n/](https://osf.io/qad4n/)). 

Our generic feature engineering steps included: 1) imputing missing data (median imputation for numeric features, mode imputation for nominal and ordinal features); 2) removing zero-variance features; 3) using a Yeo-Johnson transformation on all numeric variables to address distributional shape; 4) dummy-coding unordered categorical variables; and 5) normalizing all features (as a requirement of the GLMNet algorithm). Medians/modes for missing data imputation and identification of zero variance features were derived from held-in (training) data and applied to held-out (validation and test) data (see Cross-validation section below). We also allowed treatment (dummy coded) to interact with all other features to permit differential prediction. 

Feature sets built with feature engineering differed in two ways across model configurations. First, ordered categorical variables (e.g., Likert scale items on self-report measures) could be ordinal scored (i.e., treated as numeric data) or dummy-coded. Ordinal scoring may decrease variance as it would create fewer features (e.g., single numeric feature ranging from 1 - 7) compared to dummy-coding (e.g., 7 dummy code features for an item with 7 response levels). However, dummy-coding allows for non-linear relationships and thus more flexibility in predictive patterns, potentially decreasing bias. Second, we built feature sets that included *either* items (i.e., individual items within a self-report measure) or scale scores (i.e., total scale and sub-scale scores derived from items in a self-report measure). Item feature sets had far more features and allowed for both linear and non-linear relationships between scale constructs and the outcome, decreasing bias. Scale feature sets had fewer features and required linear relationships only, decreasing variance.

##### Dimensionality reduction

Dimensionality reduction approaches offer additional ways to produce lower-variance solutions that may generalize better to new data. We used several data-driven methods for dimensionality reduction such as: removing near-zero variance features, removing highly correlated features (via GLMNet penalization), and including lower-dimensional feature sets. We also used several non-data-driven approaches for dimensionality reduction. First, we removed variables that conflicted with our ultimate implementation goals (e.g., variables whose assessment required blood work or lab tests). This reduced the overall number of features in the model. Second, we removed variables that lacked face validity for predicting abstinence (e.g., <!-- find example -->) or overlapped conceptually with other features (e.g., including body mass index [1 feature] instead of height and weight [2 features]). 

<!-- removed resampling approaches, no longer considering anything except no resampling -->

#### Model fitting, selection, & evaluation

##### Cross-validation

Cross-validation allows for rigorous consideration of many model configurations and prioritizes performance in new data not used for model training [@jonathanUseCrossvalidationAssess2000]. Specifically, we used nested cross-validation for model training, selection, and evaluation [@krstajicCrossvalidationPitfallsWhen2014]. 

Nested cross-validation uses two nested loops for dividing and holding out folds: an outer loop, where held-out folds serve as *test sets* for model evaluation; and inner loops, where held-out folds serve as *validation sets* for model selection. Importantly, these sets are independent, maintaining separation between data used to train the models (held-in data), select the best models (held-out validation sets), and evaluate those best models (held-out test sets). Therefore, nested cross-validation removes optimization bias from the evaluation of model performance in the test sets and can yield lower variance performance estimates than single test set approaches [@jonathanUseCrossvalidationAssess2000; @krstajicCrossvalidationPitfallsWhen2014].

We used 1 repeat of 10-fold cross-validation for the inner loops and 3 repeats of 10-fold cross-validation for the outer loop. Best model configurations were selected using median model performance across the 10 *validation sets*.  Final performance evaluation of those best model configurations used median performance across the 30 *test sets*.  

##### Performance metric

Our primary performance metric for model selection and evaluation was area under the Receiver Operating Characteristic Curve (auROC) [@kuhnAppliedPredictiveModeling2018; @youngstromPrimerReceiverOperating2014]. In simplest terms, auROC measures how well our model discriminates between positive (abstinent) and negative (smoking) cases. More specifically, auROC indexes the probability that the model will predict a higher score for a randomly selected positive case relative to a randomly selected negative case. This metric was selected because it 1) combines sensitivity and specificity, which are both important characteristics for clinical implementation; and 2) is unaffected by class imbalance. 

We report median auROC for our best model configurations in the test sets. For completeness, we also report auROCs from the validation sets in the Supplement.  

##### Bayesian analysis of model performance

We used a Bayesian hierarchical generalized linear model to estimate the posterior probability distributions and 95% Bayesian credible intervals (CIs) for auROC for the best models. Posterior probability is the likelihood of obtaining our results given our data. A posterior probability distribution around a given parameter (e.g., median auROC) allows us to assess the certainty of our results.

We estimated the posterior probability distribution around model performance following recommendations from the tidymodels team [@kuhnTidyposteriorBayesianAnalysis2022]. We regressed the auROCs (logit transformed) from the 30 test sets on two random intercepts: one for the repeat, and another for the fold within repeat (folds are nested within repeats for 3X10-fold cross-validation). We report the 95% (equal-tailed) Bayesian CIs from the posterior probability distributions for our model's auROCs. If 95% Bayesian CIs do not include 0.5 (chance performance), we can conclude that the model performs better than chance. 

Bayesian analyses were accomplished using the tidyposterior [@kuhnTidyposteriorBayesianAnalysis2022] and rstanarm [@goodrichRstanarmBayesianApplied2023] packages in R. Following recommendations from the rstanarm team and others [@rstudioteamRStudioIntegratedDevelopment2020; @gabryPriorDistributionsRstanarm2023], we used the rstanarm default autoscaled, weakly informative, data-dependent priors that take into account the order of magnitude of the variables to provide some regularization to stabilize computation and avoid over-fitting. Specifically, the priors were set as follows: residual standard deviation ~ normal(location=XX, scale=exp(XX)), intercept (after centering predictors) ~ normal(location=XX, scale=XX), the two coefficients for window width contrasts ~ normal (location=XX, scale=XX), and covariance ~ decov(regularization=XX, concentration=XX, shape=XX, scale=XX). <!-- UPDATE PRIORS, i don't know if i use specified priors or adjusted priors -->

#### Model interpretation

Interpretability is important for several reasons when using machine learning approaches for clinical applications [@maceachernMachineLearningPrecision2021; @mooneyBigDataPublic2018; @cohenTreatmentSelectionDepression2018]. First, understanding how the model is making predictions, including identifying important features, can help advance theory. For example, determining factors that predict smoking cessation may help to guide novel treatment development by identifying maintaining mechanisms or predictors of success. Second, interpretability may help support implementation, as patients or clinicians may be more likely to use a model that they understand. Although it is important not to conflate interpretability with trust, it offers a step in the right direction.

We pursued model interpretation in two ways. First, the GLMNet algorithm offers several advantages for interpretability. It is a linear model that outputs parameter estimates for each feature. It also performs regularization by shrinking parameter estimates and/or removing unnecessary or highly correlated features from the model entirely. Thus, features are retained in the model only if their contribution to performance outweighs the cost of having an additional parameter in the model. Consequently, we can review the retained features and their parameter estimates as a metric of feature importance.

Second, we computed Shapley Values [@lundbergUnifiedApproachInterpreting2017] to provide a consistent, objective explanation of the importance of features. Shapley values possess several useful properties including: Additivity (Shapley values for each feature can be computed independently and summed); Efficiency (the sum of Shapley values across features must add up to the difference between predicted and observed outcomes for each observation); Symmetry (Shapley values for two features should be equal if the two features contribute equally to all possible coalitions); and Dummy (a feature that does not change the predicted value in any coalition will have a Shapley value of 0).

We calculated Shapley values by conducting leave-one-out cross-validation using the final, best selected model configuration. Leave-one-out cross-validation works identically to *k*-fold cross-validation described above. The value of *k* is set to *N* (sample size) such that each test set consists of a single held-out participant. Thus, we fit *N* = 1086 models, where each participant served as the test set once for a model fit with the other 1085 participants. This method allowed us to calculate Shapley values in held-out data, while ensuring our model stayed as close as possible to the final model (using the full dataset) that we would disseminate going forward (see "Select final model configuration" below).

We used the DALEX and DALEXtra packages [@biecekDALEXExplainersComplex2018] in R, which provide Shapley values in log-odds units for binary classification models. These Shapley values estimate local importance (i.e., for each observation). To calculate global importance (i.e., across all observations), we averaged the absolute value of the Shapley values of each feature across observations. These local and global importance scores based on Shapley values allow us to answer questions of *relative feature importance*; however, these are descriptive analyses because standard errors or other indices of uncertainty for importance scores are not yet available for Shapley values.

### AIM 2 analytic strategy

We followed methods described and used previously for evaluating the potential clinical utility of treatment selection models [@derubeisPersonalizedAdvantageIndex2014; @derubeisHistoryCurrentStatus2019]. We also preregistered our analyses to evaluate clinical benefit; the preregistration can be found on our OSF page ([https://osf.io/qad4n/](https://osf.io/qad4n/)). 

#### Select final model configuration

Following model evaluation, we completed another round of 1 repeat of 10-fold cross-validation using the full dataset. A single best model configuration was selected using median auROC across the 10 held-out folds. Importantly, model performance in this phase was used *only* for selection and not evaluation [@krstajicCrossvalidationPitfallsWhen2014]. This best selected model configuration was then used for clinical benefit analyses. 

A final model was fit on the full dataset using this best selected model configuration. This model was used to obtain parameter estimates to aid with interpretation, and it would serve as the model disseminated for ultimate implementation.

#### Identify model-predicted best treatment

As we did for feature importance with Shapley values, we conducted leave-one-out cross-validation such that we fit 1086 models using the best model configuration (selected in 1x-10-fold cross-validation) with each participant held-out from model fitting once. For each participant, we fit a model using the other 1085 participants and made predictions for the single held-out participant. This ensured that we were making predictions for a new patient (i.e., one that our model had not seen) to match most closely how this model would be ultimately implemented. 

We calculated three predicted probabilities for each participant by substituting each treatment into the model inputs. Thus, there is one prediction per person per treatment. These substitutions affected the model's predicted probabilities through any main effect of treatment and any interactions of treatment with other features. The treatment that yields the highest model-predicted probability of abstinence is identified as that participant's "best" treatment. 

For example, an individual may have received varenicline in the original trial. We calculated their probability of abstinence using their data (i.e., with varenicline as the "treatment" feature), and then we calculated two additional probabilities by substituting C-NRT and nicotine patch for varenicline. Among these three predicted probabilities, their probability of abstinence may be highest when C-NRT is substituted in as their treatment. This would mean C-NRT is identified as the best treatment for that individual.

#### Categorize treatment matching

Some participants' best treatment matched what they were randomly assigned in the original trial. Other participants may have received a sub-optimal treatment (i.e., what the model identified as their second-best or worst treatment based on calculated probabilities). Thus, participants' RCT-assigned treatment can be categorized by whether it "matched" their model-selected treatment. 

For example, the individual described above received varenicline in the original trial, but their model-predicted probability of abstinence was highest when C-NRT was substituted in as their treatment. This participant's best treatment did not match their trial treatment, so they would be labeled as "unmatched."

#### Evaluate clinical benefit

Our primary analysis to evaluate the clinical benefit of our model-selected treatment compared the observed outcomes (i.e., abstinence vs. smoking from the original trial) for people who did or did not receive their best treatment (i.e., matched or unmatched). Treatment matching was thus a between-subjects predictor and was coded as 0.5 (TRUE, "matched") vs. -0.5 (FALSE, "unmatched"). 

We examined this effect over time at 4, 12, and 26 weeks by allowing the effect of treatment match to interact with time (i.e., week). Time was a within-subjects variable with three repeated measures for each participant. We treated time numerically, and we used a log transformation to meet linearity assumptions. We preregistered using a log transformation with base e; however, due to issues of convergence, we switched to log base 2. 

Our model included treatment match, time, the interaction between treatment match and time, a by-subject random slope for time, and a by-subject random intercept. We followed a mixed-effects modeling approach using the blme package [@chungNondegeneratePenalizedLikelihood2013a]. Specifically, we fit a partially Bayesian generalized linear model that uses regularizing priors to force the estimated random effects variance-covariance matrices away from singularity [@chungNondegeneratePenalizedLikelihood2013a]. If the interaction between treatment match and time was significant, we planned to conduct follow-up tests of the simple effect of treatment match at each time point (week 4, week 12, and week 26) using general linear models. 

We identified the main effect of treatment match *a priori* as our focal effect; however, we report all estimates, test statistics, *p*-values, and confidence intervals from all models. 

## Results

<!-- need to update values & insert figure numbers -->

### Sample characteristics

* analysis sample inclusion criteria and final sample size here (full sample)
* descriptive statistics on demographics and maybe some tobacco-related characteristics
* tables

<!--KW: It might be helpful to use subheadings that match your aims (like how you have your Methods) - e.g., AIM 1 analytic strategy: Model building and AIM 2 analytic strategy: Evaluation of clinical benefit. -->

### Model performance

We selected the best model configurations using auROCs from the *validation sets*. We report the median and IQR auROCs from the validation sets for these best model configurations in the Supplement. We evaluated these best model configurations using *test set* performance to remove optimization bias present in performance metrics from validation sets [krstajicCrossvalidationPitfallsWhen2014].

The median auROC across the 30 test sets for the 4-week model was 0.682 (IQR = 0.654 - 0.712, range = 0.589 - 0.797). The median auROC across the 30 test sets for the 26-week model was 0.645 (IQR = 0.605 - 0.672, range = 0.513 - 0.762). These values are comparable to model performance from extant literature predicting smoking cessation using machine learning (e.g., auROC = 0.660 [@laiDevelopmentMachineLearning2021]). Additional performance metrics (not used for selection or primary evaluation) are reported in the Supplement.

We used the 30 test set auROCs to estimate the posterior probability distribution for the auROC of these models. The median auROCs from these posterior distributions were 0.687 (4-week model) and 0.639 (26-week model). These values represent our best estimates for the magnitude of the auROC parameter for each model. The 95% Bayesian CI for the auROCs were relatively narrow and did not contain 0.5 (chance performance) for either the 4-week model [0.666 - 0.707] or the 26-week model [0.617 - 0.661]. Figure X displays posterior probability distributions for the auROC for the models by outcome.

<!-- move to supp
#### Bayesian model comparisons

We used the posterior probability distributions for the auROCs to compare formally the 4- and 26-week models. The median increase in auROC for the 4- vs. 26-week model was 0.047 (95% CI = 0.030 - 0.065), yielding a probability of 100% that the 4-week model had superior performance. Figure X presents histograms of the posterior probability distribution for this model contrast.  

is the probability allowed to be 100%...?
-->

### Feature importance

#### Parameter estimates for retained variables

Table X presents the retained features from the best 4-week model configuration and their parameter estimates. This model retained 128 features (best model configuration alpha = 0.1, item feature set). Of the 128 retained features, 56 were treatment interaction features, suggesting the importance of these interactions for prediction. To perform treatment selection, only interactive features would need to be assessed, as features that increase or decrease probability magnitude equally across all three treatments do not help with differential prediction. Consequently, implementing this model for treatment selection would require assessing only 37 unique items (e.g., multiple dummy variables are from a single item, a feature interacts with two levels of treatment). 

Table X presents the retained features from the best 26-week model configuration and their parameter estimates. This model retained 38 features (best model configuration alpha = 0.3, scale feature set). Of the 38 retained features, 10 were treatment interaction variables, suggesting the importance of these interactions for prediction. To implement this model for treatment selection, this model would require assessing only 13 unique items (some retained items constituted scale scores and required assessing multiple items to calculate). 

#### Shapley values

Global importance (mean |Shapley value|) for features for each model appear in Panel A of Figure X. XX was the most important feature across prediction outcomes. XX, XX, and XX were also globally important across models. XX, XX, and XX were the most relatively important treatment interaction variables. XX was globally important for only the 4-week model, whereas XX was globally important for only the 26-week model.

<!-- not sure if i will need these? less relevant for my context/purpose --> Sina plots of local Shapley values (i.e., the influence of features on individual observations) for each model show that some features (e.g., XX, XX, XX) impact abstinence probability for specific individuals even if they are not globally important across all observations (Figure X, Panels B-C).

### Clinical benefit

<!-- redo values once predictions have been made with LOOCV & redo model -->

There was a significant fixed effect of treatment matching on abstinence (OR = , *z* = 5.640, *p* < 0.001). Individuals who received their model-predicted best treatment were more likely to be abstinent. There was also a significant fixed effect of time (OR = , *z* = -9.948, *p* < 0.001) such that the probability of abstinence declined over time. 

There was not a significant interaction between treatment matching and time (*p* = 0.830). However, calculating and interpreting interactions in logistic models is not straightforward because significance can differ based on the link function used [@karaca-mandicInteractionTermsNonlinear2012; @collinsOptimizationBehavioralBiobehavioral2018]. Consequently, we conducted simple effects analyses of the effect of treatment matching at each time point <!-- add a footnote that this differed from our preregistration (models/analysis code the same as preregistered, but we said we'd do them only if interaction was significant) -->. This allowed us to characterize our results more fully and to understand our effects in their original probability terms.

There was a significant fixed effect of treatment matching on abstinence at 4 weeks (OR = , *z* = , *p* = ) and at 12 weeks (OR = , *z* = , *p* = ). The effect of treatment matching was no longer significant by the 26-week follow-up assessment (*p* = ). Figure X shows the mean abstinence rate by treatment matching at each time point.



## Discussion

<!-- 
rather than having its own section:
1. weave throughout (eg talking about vulnerable pops in smoking cessation tx, genetics/bio variables inequitable)
2. implementation focus (rather than equity solely)
3. discussion section including future directions re: algorithmic fairness

Critically, the combination of machine learning methods with the precision mental health paradigm may be able to mitigate health disparities in our current treatment pipeline [@maceachernMachineLearningPrecision2021]. Treatments are rarely designed or evaluated in diverse samples; input directly from patient shareholders is incorporated even less often. Although the NIH has launched some new initiatives to improve effort in these areas, when it comes to treatments, we still rely almost exclusively on treatments developed decades ago that were effective in homogeneous samples. KW: I would add parenthetical example of what this homogenous sample looked like (e.g., white, male, middle class)

In some ways, these problems have been amplified by early work in precision medicine and precision mental health. Studies often begin with retrospective samples before bringing the models built in those samples to new patients. Thus, these models may be likely to fail for the same people for whom our initial treatments fail - the model can only be as good as the data with which it was developed [@aldridgeResearchTrainingRecommendations2019]. However, when this research is done thoughtfully, there are clear opportunities to address health disparities.

First, we can make a concerted effort to build treatment selection models using data from previously completed trials where there was good representation across as many marginalized characteristics as possible. This may include demographic variables such as sex, gender identity, race, or ethnicity, among others. Other important characteristics to consider include income, access to insurance, and geographic region.

In some contexts, it would be problematic to use predictors that tap into constructs delineating marginalized identities such as race or socioeconomic status. For example, making decisions about who gets insurance (or doesn't) and who gets released earlier from prison (or doesn't) based on race would be discriminatory (e.g., [@farayolaEthicsTrustworthinessAI2023]). However, in the precision mental health landscape, we are not deciding *who* gets treatment. Rather, we are deciding *which* treatment to give a specific patient. Thus, we can take advantage of experiential or symptomatological differences as a result of characteristics such as race, ethnicity, sex, income, or comorbid health conditions to improve treatment outcomes across vulnerable subpopulations. keep a lot of this paragraph but tbd where

An additional way in which this approach can help mitigate health disparities is through access. Access to treatment is a known barrier in mental healthcare and a contributing factor driving mental healthcare disparities [@jacobsonDigitalTherapeuticsMental2022]. Many individual difference features that could help build treatment selection models may be easily measured via self-report, and dimensionality reduction approaches employed within machine learning can limit the number of features that need to be assessed. Consequently, treatment selection models might require sparse assessment of only a handful of readily available items and could even be implemented online. In cases where treatments are available over-the-counter, completing the assessment remotely means that individuals without insurance or access to in-person medical care can still give themselves the best chance of treatment success.
-->

<!-- IGNORE FOR NOW

we are not in the business of picking smoking cessation meds, we are in the business of improving treatments

discussion: remember to discuss chronic diseases, need for time-varying tx and personalizing/adapting OVER TIME cite oncology vs addiction paper

from nrsa innovation & impact:

The proposed project combines machine learning with precision mental health to select among treatments for cigarette smoking cessation. Machine learning remains novel and underused in substance use and, to some degree, clinical psychology broadly16. Further, prior machine learning research for SUDs has prioritized diagnosis and screening17–20 over treatment selection, highlighting the novelty of this project.

This project is also innovative in that it will consider a wide variety of features within and across domains (e.g., smoking history, other substance use, psychological traits/conditions, physical health, demographics, social environment). Machine learning methods allow me to incorporate these features simultaneously and build high-dimensional models that capture real-world complexities. Including features across domains invites interactions among features and subsequent integration of knowledge across domains. The proposed project will allow features often considered disparate to interact with one another, permitting combinations of features that are both guided by domain expertise and driven by less intuitive patterns in the data.

This project could provide immediate impact by offering a decision-making tool for selecting among smoking cessation treatments. My model would provide patient-level guidance for treatment selection among smoking cessation treatments, which would reduce health risks and societal burdens. Critically, this approach could offer those benefits without requiring new treatment development, which is time- and resource-intensive. Instead, we can improve treatment effectiveness by optimizing therapeutic benefit of existing treatments. 

I aim to increase potential impact by allowing feasibility to guide model development. I will prioritize sparse models with readily available, easy-to-measure features to the degree that these models yield robust treatment selection. The available features set can be easily measured via self-report. Easy measurement would allow our decision-making tool to be implemented without blood draws, expensive or otherwise inaccessible testing (e.g., neuroimaging), formal clinical interviewing, or even in-person doctors’ visits. In fact, prioritizing feasibility is a primary rationale for excluding genetic features from the present project; genetic features would require specialty materials and/or in-person medical appointments during clinical implementation. An assessment constructed entirely from self-report measurements is particularly valuable because two treatments in the model (nicotine patch, combination NRT) are widely available over the counter, offering scalable implementation even when medical access is limited. This focus on feasibility (i.e., using low burden, low-cost features) could maximize potential clinical benefit. These efforts are especially warranted when applying the precision mental health paradigm to SUDs, for which large health disparities on the bases of race, ethnicity, socioeconomic status, and geographic location persist.

lots of stuff in baker arcp re: meds are not nearly as effective in real-world settings. NEED LONGER TERM SUPPORT!!!

main effect of time also supports need for continuing care

notes from john meeting:

for actual paper, everything 26 week in supp including model comparison
issue of time in discussion = related to aim 2
predicting later out makes less sense & doesn't perform as well
for diss, also everything in supp
can put "discussion" section in supp as well to interpret 26-week model aim 1 and aim 2 results
in methods: one sentence comparable analyses with 26 week outcome for models, performed generally less well (could even be footnote), but reporting everything in the supplement

take model comparison out of methods/results

parameter estimate tables into supplement, documentation of how you would use the model -> get these from fit_final_model scripts
do keep mention of how many features retained and how many are tx interactions

don't report secondary metrics

calibration plot main text with model performance
roc curve (TP y-axis by FP x-axis rate, both go from 0 to 1) as figure; add roc curve cite back in -> 2 panel plot with posterior distribution as second panel

concatenate 30 held-out ROC curves (for display purposes)
- maybe supp figure for 30 roc curves

30 held-out aurocs in supplement histogram

outline for discussion: start today in lab meeting & then follow-up with john later this week

discussion notes from lab meeting:
1. this works!!!! can really be helpful!
2. ok that auroc isn't that high because it's not our end goal
3. might be able to predict better if we incorporated things like bio & genetic markers but comes at implementation cost
4. tandem: tx development alongside pmh, who it doesn't work for, what factors predict success
5. need adaptive support over time (no benefit at 26 week, can't predict that well overall using only baseline characteristics)

MAKING A DISTINCTION BETWEEN MODELS THAT PREDICT BETTER AND MODELS THAT PREDICT DIFFERENTIALLY BETTER - a model that predicted abstinence perfectly but included no interactions would be useless for tx selection goal

looking at "main effect" prediction may help identify mechanisms to target with other meds/new tx/etc.

start with 37 unique things to assess - why do we only need to assess those and not main effects - difference in goals of overall prediction vs selection of tx - equity/implementation

harm reduction in smoking?

-->

<!-- TBD:
The additivity property of Shapley values allowed us also to examine the relative importance of categories of features. We created two kinds of feature categories. First, we examined feature importance within a single self-report measure (e.g., all features derived from items from a specific measure). Second, we examined feature importance within a domain: demographic features, mental health features, physical health features, smoking use history, and social/environmental features. To calculate the local importance for each category of features, we added Shapley values across all features in a category, separately for each observation. To calculate global feature importance for each feature category, we averaged the absolute value of the Shapley values of all features in the category across all observations.

nts: "domain" grouping (second idea in above paragraph) was used similarly by PATH paper, and i liked it there
-->

<!-- notes on unique item assessment:

4-WEEK BEST MODEL 

unique items required for overall prediction: 

live with smoker (MC), 
race, 
wisdm 2 & 5 & 13 & 14 & 16 & 18 & 19 & 20 & 22 & 26 & 27 & 30 & 32, 
smoke menthol y/n, 
used cigars (MC), 
close smoking friend y/n, 
dts 5 & 13 & 15, 
asi3 2 & 9 & 15, 
spouse smoke (MC), 
close smoking coworker y/n, 
dsm5 3 & 6 & 7 & 8 & 10 & 12, 
marital status, 
mfi 5 & 6 & 9 & 13 & 15, 
close smoking relative y/n, 
income, 
depression dx, 
work ban (MC), 
ftnd 1 & 2 & 6, 
berlin 7, 
tried nicotine gum y/n, 
wsws constipation & want cigarette & unhappy & 18 & 5 & 9 & angry & stressed & 12 & crave & 26 & coughing, 
life satisfaction, 
used pipe (MC), 
gender, 
carbon monoxide, 
motive for quitting, 
longest quit, 
time around smokers on weekend, 
shp 2 & 3 & 9 & 13 & 14, 
total quit attempts, 
importance to quit, 
sip2r 2 & 3 & 4, 
hdsm tense & pain
time around smokers
30-day quit success
phq9 6
used chew or snuff (MC)
used ecig (MC)
hrqol 4

- total: 83 unique items required for overall prediction

unique items required for differential prediction: 

live with smoker (MC), 
race, 
wisdm 16 & 18 & 19 & 22 & 26 & 27 & 30 & 32, 
smoke menthol y/n, 
used cigars (MC), 
close smoking friend y/n, 
dts 5, 
asi3 2 & 15, 
spouse smoke (MC), 
close smoking coworker y/n, 
dsm5 3 & 6 & 7 & 10, 
marital status, 
mfi 6 & 13, 
close smoking relative y/n, 
income, 
depression dx, 
work ban (MC), 
ftnd 2 & 6, 
berlin 7, 
tried nicotine gum y/n, 
wsws constipation, 
life satisfaction, 
used pipe (MC), gender

- total: 37 unique items required for treatment selection

(potential for some items to be combined eg people close to you who smoke, check all that apply)

26 WEEK BEST MODEL

unique items required for overall prediction:

marital status
live with smoking spouse (MC)
live with smoker (MC)
tried nic gum previously
asi3 cognitive scale (6 items), social scale (6)
close smoking partner Y/N
close smoking friend Y/N
used e cigs (MC)
co
wisdm37 en_goad scale (3), tolerance scale (4), cue scale (3), auto scale (4)
race
motivation to quit
smoke menthol cigs y/n
shp total scale (14)
wsws negative affect scale (4), crave scale (3)
longest prev quit
total # quit attempts
dsm5 score (11 items)
age 1st cig
age
hdsm tired item
time around smokers (weekdays, weekends)

- requires assessing 76 unique items for overall prediction

unique items required for treatment selection:

marital status
live with smoking spouse (MC)
live with smoker (MC)
tried nic gum previously
asi3 cognitive scale (6 items)
close smoking partner Y/N
close smoking friend Y/N
used e cigs (MC)

- requires assessing 13 unique items for tx selection

-->