---
title: Machine learning-assisted treatment selection for smoking cessation
author:
  - name: Gaylen E Fronk
    orcid: 0000-0001-6653-9699
    corresponding: false
    roles: []
    affiliations:
      - Department of Psychology, University of Wisconsin-Madison
  - name: Kendra Wyant
    orcid: 
    corresponding: false
    roles: []
    affiliations:
      - Department of Psychology, University of Wisconsin-Madison
  - name: Megan E Piper
    orcid: 
    corresponding: false
    roles: []
    affiliations:
      - Center for Tobacco Research & Intervention, University of Wisconsin-Madison
  - name: Timothy Baker
    orcid: 
    corresponding: false
    roles: []
    affiliations:
      - Center for Tobacco Research & Intervention, University of Wisconsin-Madison
  - name: John J. Curtin 
    orcid: 0000-0002-3286-938X
    corresponding: true
    email: jjcurtin@wisc.edu
    roles: []
    affiliations:
      - Department of Psychology, University of Wisconsin-Madison 
keywords:
  - Substance use disorders  
  - Precision mental health  
  - Cigarette smoking  
  - Machine learning  
  - Treatment selection  
abstract: |
  Precision mental health seeks to select the right treatment for a patient given personal characteristics. The purpose of this project was to build a machine learning model that could select among first-line medication treatments for cigarette smoking. We used data from a previously completed comparative effectiveness trial in which participants were richly characterized at baseline before being randomly assigned to varenicline, combination nicotine replacement therapy, or nicotine patch. We built a model predicting treatment success (abstinent vs. smoking) using baseline characteristics and their interactions with treatment. Models were fit, selected, and evaluated using nested cross-validation and the performance metric area under the receiving operator characteristic curve (auROC). Our best models had a median auROC of 0.69 in held-out test sets. We used this model to calculcate probabilities of treatment success for each participant on each of the three treatments to identify their model-predicted best treatment. Individuals who were matched to their model-predicted best treatment during the original trial were more likely to quit successfully at 4 weeks than individuals who were not (OR = 1.382, p = 0.014) but not at later timepoints. This project produces a clinically implementable treatment selection model to assist people quitting cigarette smoking. 
key-points:
  - Take away point 1 
  - Take away point 2
date: last-modified
bibliography: references.bib
bibliographystyle: apa 
number-sections: false 
editor_options: 
  chunk_output_type: console
---

<!--nts: abstract 187 words -->

## Introduction


Precision medicine seeks to guide and personalize treatment selection using individual difference characteristics that are likely to predict treatment success for each patient [@bickmanImprovingMentalHealth2020]. Successful precision medicine would increase the likelihood of treatment success for each patient because each patient receives the treatment predicted to work best for them. It would also improve treatment effectiveness rates across the population because each treatment is administered only to the patients for whom that treatment is expected to be their best option. 

Precision mental health is the application of the precision medicine paradigm to mental health conditions [@derubeisHistoryCurrentStatus2019; @inselNIMHResearchDomain2014; @bickmanAchievingPrecisionMental2016; @bickmanImprovingMentalHealth2020]. Precision mental health models may allow us to select among these treatments to improve mental healthcare. Treatments for mental health conditions are usually no more than moderately effective, and many treatments for the same disorder can be quite comparable [@lewisPsychologicalTherapiesPosttraumatic2020; @weiszArePsychotherapiesYoung2019; @adjeiCO110ComparativeEffectiveness2022]. Consequently, many researchers have pursued precision mental health in recent years [@derubeisHistoryCurrentStatus2019]. 

Despite the abundance of research, much less progress has been made to personalize treatments for mental health disorders compared to treatments for medical disorders [@bickmanAchievingPrecisionMental2016; @bickmanImprovingMentalHealth2020; @kranzlerPrecisionMedicinePharmacogenetics2017; @oliverPrecisionMedicineAddiction2017; @kesslerPragmaticPrecisionPsychiatry2021]. One possible reason for this is that many factors influence heterogeneous, complex clinical phenomena like mental health diagnoses and treatment success [@feczkoMethodsChallengesAssessing2020]. Thus, any single feature (i.e., predictor variable) cannot account for more than a small portion of the variance in treatment success [@kesslerPragmaticPrecisionPsychiatry2021; @inselNIMHResearchDomain2014]. Precision mental health efforts so far, however, have largely focused on personalizing treatments using only a single feature [@derubeisHistoryCurrentStatus2019]. It is perhaps unsurprising that models that consider only one or a small handful of features - which also limits considering features from different categories concurrently - have failed to capture the real-world complexity underlying clinical phenomena like treatment success.

Additionally, because models are typically developed and evaluated in the same sample, the models may become very overfit to that sample [@jonathanUseCrossvalidationAssess2000]. This problem is exacerbated in precision mental health because sample sizes in psychological research have remained relatively small despite recommendations to increase sample size [@marszalekSampleSizePsychological2011]. Consequently, precision mental health models rarely generalize well to new patients. 

To capture sufficient complexity to predict treatment success, we need to increase the total number of features in precision mental health models. Incorporating more features, however, makes overfitting the data more likely. Thus, successful precision mental health requires an analytic approach that can handle high-dimensional data without becoming too overfit to generalize to new patients.

### Applying machine learning approaches


Machine learning may be able to advance precision mental health goals [@bickmanAchievingPrecisionMental2016; @dwyerMachineLearningApproaches2018; @maceachernMachineLearningPrecision2021; @mooneyBigDataPublic2018]. These models use statistical algorithms trained on high-dimensional arrays (hundreds or even thousands) of features [@jamesIntroductionStatisticalLearning2013; @kuhnAppliedPredictiveModeling2018; @ngMachineLearningYearning2018]. Machine learning uses various techniques (e.g., regularization, hyperparameter tuning, simultaneous consideration of many model configurations) within resampling methods such as cross-validation to accommodate high-dimensional sets of features while reducing overfitting [@jamesIntroductionStatisticalLearning2013; @kuhnAppliedPredictiveModeling2018; @ngMachineLearningYearning2018; @mooneyBigDataPublic2018; @krstajicCrossvalidationPitfallsWhen2014; @jonathanUseCrossvalidationAssess2000]. Thus, we can build precision mental health models that capture complex clinical phenomena and generalize accurately to new data. 

### Cigarette smoking as a critical precision mental health target


Combining precision mental health and machine learning may be able to improve treatment for cigarette smoking. Smoking remains an enormous public health burden. Tobacco is the leading cause of preventable death in the U.S., accounting for more than 480,000 deaths annually [@nationalcenterforchronicdiseasepreventionandhealthpromotionusofficeonsmokingandhealthHealthConsequencesSmoking2014; @schlamInterventionsTobaccoSmoking2013; @corneliusTobaccoProductUse2020]. Individuals who smoke have two- to three-fold likelihood of death across causes and lose over a decade of life expectancy [@jhaprabhat21stCenturyHazardsSmoking2013]. Although rates of smoking have declined considerably, approximately 14% of U.S. adults continue to smoke daily or near-daily [@corneliusTobaccoProductUse2020; @substanceabuseandmentalhealthservicesadministrationKeySubstanceUse2023]. Additionally, cigarette smoking rates remain much higher in potentially vulnerable populations. This includes people with chronic or severe mental illness including other substance use disorders; Native American and non-Hispanic Black individuals; individuals who are economically and educationally disadvantaged; people in the criminal legal system; people experiencing homelessness; individuals who are uninsured or insured through Medicaid; and people who identify as lesbian, gay, or bisexual [@bakerSmokingTreatmentReport2021; @jamalCurrentCigaretteSmoking2015a; @corneliusTobaccoProductUse2020; @kellyPrevalenceSmokingOther2012; @cropseySmokingFemalePrisoners2004; @harrisonCigaretteSmokingMental2020; @baggettTobaccoUseHomeless2013; @soarSmokingAmongstAdults2020].

The best available smoking cessation treatments are modestly effective. The medications varenicline and combination nicotine replacement therapy (C-NRT) are the most effective options when combined with psychosocial counseling, yielding 6-month treatment success rates of 30-35% [@cahillPharmacologicalInterventionsSmoking2013; @schlamInterventionsTobaccoSmoking2013; @rigottiTreatmentTobaccoSmoking2022; @fioreClinicalPracticeGuideline2008; @bakerSmokingTreatmentReport2021]. Varenicline and C-NRT appear to be equally effective  [@cahillPharmacologicalInterventionsSmoking2013; @bakerEffectsNicotinePatch2016]. Indeed, national clinical guidelines note that “there are no well-accepted algorithms to guide optimal selection” between any of the first-line medications ([@fioreClinicalPracticeGuideline2008], p. 44).<!--KW: not sure how to format reference with page number but double (( are being rendered as is. I wonder if [@fioreClinicalPracticeGuideline2008, p. 44] would work? GEF: known issue, will follow up with Susan about if she has found a solution. may just need to manually edit in word for submission --> These facts suggest a critical need for precision mental health approaches in the cigarette smoking domain.

#### Previous research


To select among treatments for smoking cessation, precision mental health models must include *many features* and *multiple treatments* as inputs. Using a high-dimensional array of features is necessary to predict a complex clinical outcome like treatment success. Including multiple treatments allows for interactions among treatments and features that are needed to make differential predictions by treatment. 

There is a large body of research identifying features that predict overall treatment success (i.e., irrespective of treatment) [@laiDevelopmentMachineLearning2021; @kaufmannRateNicotineMetabolism2015; @kayeSearchingPersonalizedMedicine2020a; @piperPrecisionSmokingCessation2017a; @issabakhshMachineLearningApplication2023; @etterPredictingSmokingCessation2023] or treatment success when using a single treatment [@massagoApplicabilityMachineLearning2024; @coughlinMachineLearningApproachPredicting2020a]. However, these models that do not include multiple treatments do not offer an actionable way forward to select among treatment options. Even research that shows who might succeed within a specific treatment has limited utility for treatment selection: what does this mean for a patient who is not predicted to succeed using that treatment? 

Some research has begun to build models that consider multiple treatments simultaneously [@chenGeneticVariantCHRNA52020; @shahabDoesNicotineMetabolite2019; @schnollNicotineMetabolicRate2009; @glatardAssociationNicotineMetabolism2017; @chenowethNicotineMetaboliteRatio2016; @lermanUseNicotineMetabolite2015; @siegelUseNicotineMetabolite2020; @kayeSearchingPersonalizedMedicine2020a; @piperIdentifyingEffectiveIntervention2016; @piperPrecisionSmokingCessation2017; @piperPrecisionSmokingCessation2017a]. However, each study examined only a single feature or examined each feature in a separate model, which cannot explain sufficient variance in complex clinical outcomes. 

#### Opportunities for treatment selection


Although there is not yet much research selecting among smoking cessation treatments, there may be reason to expect that some treatments may work better than others for a specific individual. First, there is enormous heterogeneity among people who smoke cigarettes [@oliverPrecisionMedicineAddiction2017; @wangSociodemographicVariabilityAdolescent2009; @zhengIdiographicExaminationDaytoDay2013]. Second, smoking cessation medications have distinct pharmacological mechanisms of action that may affect how helpful they are for different people [@cahillNicotineReceptorPartial2016; @jordanDiscoveryDevelopmentVarenicline2018; @liebermanDopaminePartialAgonists2004]. Third, the many behavioral and environmental features that have been shown to predict overall treatment success may also guide treatment selection, alone or in combination with medication mechanisms of action [@bickelPredictorsSmokingCessation2023]. Finally, there is some evidence of differential effectiveness, adherence, and preference for treatments across individuals [@heckmanEffectivenessSwitchingSmokingCessation2017; @fioreClinicalPracticeGuideline2008; @tonnesenRecyclingNicotinePatches1993; @gonzalesRetreatmentVareniclineSmoking2014; @cropseyPilotTrialVivo2017; @lindsonDifferentDosesDurations2019]. 

## Purpose


The goal of this project was to evaluate the clinical benefit of a treatment selection model. Specifically, we assessed whether using our model to select among medications for smoking cessation can increase abstinence following quitting smoking. We also assessed whether any treatment benefit is fair such that it performs equitably across demographic groups. To increase confidence in any clinical benefit we observed, we also evaluated model performance (i.e., is there predictive signal?) and interpreted our model (i.e., how does our model work?).

## Methods

### Transparency & openness


We adhere to research transparency principles that are crucial for robust and replicable science. We reported how we determined the sample size, all data exclusions, all manipulations, and all study measures. We provide a transparency report in the supplement <!-- need to do this! -->. Finally, our data and other study materials are publicly available on our [OSF page](https://osf.io/qad4n/), and our annotated analysis scripts and results are publicly available on our [study website](https://jjcurtin.github.io/study_match/). 
<!--  does this site include the pre-reg we didnt follow? GEF: the OSF page does. need to think about how to give access to data maybe without linking to OSF -->

### Data


The data for this project came from a completed randomized controlled trial conducted by the University of Wisconsin (UW) Center for Tobacco Research and Intervention (CTRI) [@bakerEffectsNicotinePatch2016]. This trial compared the effectiveness of three cigarette smoking cessation treatments (varenicline, combination nicotine replacement therapy [C-NRT], and nicotine patch). Briefly, 1086 adults who smoked cigarettes daily and were looking to quit smoking were enrolled in Madison, WI, USA and Milwaukee, WI, USA. Smoking was biochemically verified. Participants were smoking approximately 17 cigarettes per day (*M* = 17.03, *SD* = 8.31) at study enrollment. Exclusion criteria included contraindicated medical (e.g., severe hypertension) or psychiatric (e.g., severe and persistent mental illness) conditions; current use of contraindicated medications; and pregnancy or unwillingness to use contraception while taking a study medication. Participants set a quit date with study staff and were enrolled for several weeks prior to the target quit date through at least 6 months following quitting smoking. 

#### Treatment conditions


Participants were randomly assigned to one of three medication conditions: varenicline, C-NRT, or nicotine patch. For varenicline, participants began medication use prior to their quit attempt, starting with 0.5 mg once daily for 3 days, followed by 0.5 mg twice daily for 4 days. On the target quit date, participants increased to 1 mg twice daily and continued this dose for 12 weeks except in response to adverse events. For C-NRT or nicotine patch, participants initiated treatment on their quit date. Patch dosing was 8 weeks of 21 mg, 2 weeks of 14 mg, and 2 weeks of 7 mg. All individuals who received C-NRT were also instructed to use at least 5 lozenges per day (2 or 4 mg nicotine lozenges determined by time to first daily cigarette) for the full 12 weeks except in the case of adverse effects.

All participants also received counseling per clinical guidelines [@fioreClinicalPracticeGuideline2008]. Participants received 5 in-person counseling sessions (pre-quit, quit day, week 1, week 4, week 12) and 1 phone counseling session (week 8). Counseling consisted of motivational interviewing/enhancement, supportive therapy, and skill training.

#### Individual difference characteristics


Participants were comprehensively assessed for individual difference characteristics prior to treatment randomization. These characteristics fall into several domains expected to relate to cigarette smoking cessation: tobacco-related (e.g., cigarettes per day), psychological (e.g., psychiatric diagnoses, distress tolerance), physical health (e.g., vital signs), social/environmental (e.g., living with another person who smokes), and demographic (e.g., age, sex). A detailed list of all available individual differences variables appears in [@tbl-featureshtml]{.content-visible when-format="html"}[@tbl-featurespdf]{.content-visible unless-format="html"}.

::: {.content-hidden unless-format="html"}

{{< embed notebooks/mak_tables.qmd#tbl-featureshtml >}} 
:::

::: {.content-hidden when-format="html"}

{{< embed notebooks/mak_tables.qmd#tbl-featurespdf >}} 
{{< pagebreak >}}   
 
[Table 1: Individual Differences (Continued)]{.center}
{{< embed notebooks/mak_tables.qmd#featurespdf2 >}} 

{{< pagebreak >}}  
[Table 1: Individual Differences (Continued)]{.center}
{{< embed notebooks/mak_tables.qmd#featurespdf3 >}} 

{{< pagebreak >}}  

[Table 1: Individual Differences (Continued)]{.center}
{{< embed notebooks/mak_tables.qmd#featurespdf4 >}} 
:::

#### Treatment success outcome


Throughout study participation, treatment success was repeatedly assessed via biologically confirmed, 7-day point-prevalence abstinence. Point-prevalence abstinence assesses for any smoking (here, in the previous 7 days) and yields a single, dichotomous outcome of "abstinent" or "smoking." Participants self-reported whether they had smoked over the past 7 days, and their report was biologically confirmed via exhaled carbon monoxide (CO). Participants were labeled as "abstinent" if their CO level was less than 6 parts per million (ppm; [@bakerEffectsNicotinePatch2016]). If participants self-reported any smoking in the past 7 days, their CO level contradicted their self-report (i.e., CO level > 6 ppm), or biological confirmation could not be confirmed, participants were labeled as "smoking." 

### Evaluating clinical benefit


The primary purpose of this project was to evaluate the clinical benefit of our treatment selection model. In other words, we determined whether using a model to select treatments for individual patients improves clinical outcomes. Details on fitting, selecting, and evaluating model performance of the treatment selection model appear in "Model building, selection, & evaluation."

#### Identify model-predicted best treatment


We used our best model configuration (i.e., combination of model characteristics; see Model Building, Selection, & Evaluation) to make treatment success predictions for each participant. Models were fit with 1085 participants, and predictions were made for the remaining, held-out participant. This approach matches real-world implementation in that we made predictions for a new patient the model has not seen. 

We calculated three predicted probabilities for each participant by substituting each treatment into the model inputs. This produced one prediction per person per treatment. The treatment that yielded the highest model-predicted probability of abstinence was identified as that participant's "best" treatment. 

For example, an individual received varenicline in the original trial. We calculated their probability of abstinence using their data (i.e., with varenicline as the "treatment" feature), and we calculated two additional probabilities by substituting C-NRT and nicotine patch for varenicline. Their probability of abstinence was highest when substituting C-NRT, meaning C-NRT was identified as their best treatment.

#### Categorize treatment matching


Some participants' best treatment matched what they were randomly assigned in the original trial. Other participants may have received a sub-optimal treatment (i.e., what the model identified as their second-best or worst treatment based on calculated probabilities). Thus, participants could be categorized as having received "best treatment" or "other treatment" in the original trial. 

For example, the individual described above received varenicline in the original trial, but their model-predicted probability of abstinence was highest when substituting C-NRT for treatment. This participant's best treatment did not match their trial treatment, so they were labeled as matched to "other treatment."

#### Analysis plan


Our primary analysis evaluated the clinical benefit of our treatment selection model by comparing the observed outcomes (i.e., abstinence vs. smoking from the original trial) for people who were matched to their best treatment or to other treatment (between-subjects; 0.5 ["best treatment"] vs. -0.5 ["other treatment"]). 

We examined this effect of treatment matching on abstinence at 4, 12, and 26 weeks post-quit. Calculating and interpreting interactions in logistic models is not straightforward because significance can differ based on the link function used and whether effects are determined in probabilities, odds, or log-odds units [@karaca-mandicInteractionTermsNonlinear2012; @collinsOptimizationBehavioralBiobehavioral2018]. Consequently, we conducted three, separate generalized logistic regression models where we regressed the outcome (abstinent vs. smoking) on treatment matching. We conducted these regressions using the lme4 package in R [@batesLme4LinearMixedEffects2015]. We also conducted sensitivity analyses using only participants who reported starting their medication (defined as any medication use reported during the first 4 weeks post-quit; N = 988).

#### Fairness benchmarks


Treatment selection models that work for only a subset of people, if implemented, could widen existing mental healthcare disparities. Consequently, we conducted fairness analyses to determine whether there was equitable clinical benefit across demographic groups. We conducted analyses separately for three dichotomized demographic groups that display higher rates of cigarette smoking and/or encounter additional barriers to receiving treatment: 1) race and ethnicity (non-Hispanic White vs. non-White); 2) income (above vs. below poverty line); and 3) sex at birth (male vs. female) [@jamalCurrentCigaretteSmoking2015a; @corneliusTobaccoProductUse2020; @pinedoCurrentReexaminationRacial2019; @olfsonHealthcareCoverageService2022; @kilaruIncidenceTreatmentOpioid2020].

<!-- GEF fix/add to this once i go through analyses -->We compared the benefit of treatment selection (i.e., matched to best treatment vs. other treatment) at each time (4 weeks, 12 weeks, 26 weeks) by demographic group. We report the parameter estimate, confidence interval, and *p*-value for the effect of demographic group in each model. 

### Model building, selection, & evaluation


Data preprocessing, modeling, and Bayesian analyses were conducted in R using the tidymodels ecosystem [@kuhnTidymodelsCollectionPackages2020]. Models were trained and evaluated using high-throughput computing resources provided by the University of Wisconsin Center for High Throughput Computing [@chtc]. Additional details about model building and evaluation appear in the Supplement.

#### Feature engineering


We followed standard practices for generic feature engineering steps (see Supplement). Treatment was one-hot-coded such that there were three features, one corresponding to each treatment (varenicline, C-NRT, nicotine patch). The three treatment features were allowed to interact with all other features to permit differential prediction. A sample feature engineering script (i.e., tidymodels recipe) containing all feature engineering steps is available on our OSF study page ([https://osf.io/qad4n/](https://osf.io/qad4n/)).

#### Model configurations


All model configurations used the statistical algorithm Elastic Net Logistic Regression (GLMNet) and differed across sensible values for the associated hyperparameters, alpha and lambda. This algorithm aligned with our application goals: GLMNet allows including interation terms explicitly (i.e., interactions between treatment and all other features), and it penalizes model complexity to reduce dimensionality and ultimately reduce the risk of overfitting. Model configurations differed by 1) whether feature sets included individual items within a self-report measure or total scale/sub-scale scores derived from items, and 2) whether ordered categorical data (e.g., Likert scale items) were ordinal scored or one-hot coded. 

#### Prediction outcome


The model prediction outcome was treatment success at 4 weeks post-quit (i.e., predicting if individuals were labeled "abstinent" or "smoking" at 4 weeks). We selected treatment success at 4 weeks in order to 1) predict an outcome that - in clinical practice - would allow individuals to pivot to a different medication earlier if their treatment was unsuccessful; and 2) evaluate the differential effects of treatment while participants were actively using the medications.^[We fit additional models predicting treatment success at 12 and 26 weeks, and these models had significantly worse performance than our primary 4 week prediction model (See Supplement).]

#### Model selection


We used nested *k*-fold cross-validation for model training, selection, and evaluation [@krstajicCrossvalidationPitfallsWhen2014]. Nested cross-validation uses two nested loops for dividing and holding out folds: an outer loop, where held-out folds serve as *test sets* for model evaluation; and inner loops, where held-out folds serve as *validation sets* for model selection. We used 1 repeat of 10-fold cross-validation (i.e., *k* = 10) for the inner loops and 3 repeats of 10-fold cross-validation for the outer loop. 

Models were selected and evaluated using area under the receiver operating characteristic curve (auROC), which indexes the probability that the model will predict a higher score for a randomly selected positive case (abstinent) relative to a randomly selected negative case (smoking) [@kuhnAppliedPredictiveModeling2018; @youngstromPrimerReceiverOperating2014]. Because it was important that our model retained interaction features, we required that model configurations retain an average of 50 or more treatment interaction features [@ngMachineLearningYearning2018]. Best model configurations were selected using median auROC among model configurations that retained a median of 50 treatment interaction features across the 10 *validation sets*. Final performance evaluation of those best model configurations used median auROC across the 30 *test sets*.

To get a single, final model configuration, we replicated our inner loop resampling (1 repeat of 10-fold cross-validation) on the full dataset. The best model configuration was selected using median auROC across the 10 held-out folds (among model configurations that retained a median of 50 treatment interaction features). This best model configuration was used to make predictions for clinical benefit evaluation (above) and feature importance analyses (see Model Interpretation). A final model was fit on the full dataset using this best configuration to obtain parameter estimates for additional interpretation (see Model Interpretation).

#### Model performance evaluation


We used a Bayesian hierarchical generalized linear model to estimate the posterior probability distributions and 95% Bayesian credible intervals (CIs) for test set auROC. We estimated the posterior probability distribution around model performance following recommendations from the tidymodels team [@kuhnTidyposteriorBayesianAnalysis2022]. The median posterior probability for auROC represents our best estimate for the magnitude of the auROC parameter for each model. If the credible intervals do not contain .5 (chance performance), this suggests our model is capturing signal in the data. 

<!--KW: John had me add a little more information that we had moved to supplement for EMA paper when describing the Bayesian model. I updated your last paragraph to include this below. However, depending on word constraints you might need to move some of the info to supplement? Also, for the two new citations introduced you already have them in the paper_match zotero library!

We used a Bayesian hierarchical generalized linear model to estimate the posterior probability distributions and 95% Bayesian credible intervals (CIs) for test set auROC. Following recommendations from the rstanarm team and others [@rstudioteamRStudioIntegratedDevelopment2020; @gabryPriorDistributionsRstanarm2023], we used the rstanarm default autoscaled, weakly informative, data-dependent priors that take into account the order of magnitude of the variables to provide some regularization to stabilize computation and avoid over-fitting.^[Priors were set as follows: residual standard deviation ~ normal(location=0, scale=exp(2)), intercept (after centering predictors) ~ normal(location=2.3, scale=1.3), the two coefficients for window width contrasts ~ normal (location=0, scale=2.69), and covariance ~ decov(regularization=1, concentration=1, shape=1, scale=1).] We set two random intercepts to account for our resampling method: one for the repeat, and another for the fold nested within repeat. The median posterior probability for auROC represents our best estimate for the magnitude of the auROC parameter for each model. If the credible intervals do not contain .5 (chance performance), this suggests our model is capturing signal in the data. 

GEF: I believe all this info is in the supplement but will confirm, thank you!
-->

#### Model calibration


We examined the calibration of our model's predictions by comparing model predictions to observed outcomes. Specifically, we compared the predicted probabilities for each individual for their original trial-assigned treatment against the observed trial treatment success. 

### Model interpretation


Interpretability is important when using machine learning for clinical applications to identify important features and encourage implementation [@maceachernMachineLearningPrecision2021; @mooneyBigDataPublic2018; @cohenTreatmentSelectionDepression2018]. We reviewed the retained features and their parameter estimates as a metric of feature importance because GLMNet only retains features whose contribution outweighs the additional complexity. 

We also computed Shapley Values [@lundbergUnifiedApproachInterpreting2017] to provide a consistent, objective explanation of feature importance. Shapley values were calculated for each held-out participant using a model fit with the other 1085 participants and the final, best selected model configuration.

We used the DALEX and DALEXtra packages [@biecekDALEXExplainersComplex2018] in R, which provide local Shapley values (i.e., for each observation) in log-odds units for binary classification models. To index global feature importance, we averaged the absolute value of the local Shapley values of each feature across observations. Shapley values are additive, which allowed us to create two feature categories, Main Effects and Interactions. We calculated global importance of each feature category by averaging the absolute value of the Shapley values of all features in the category across observations. These global importance scores allowed us to contextualize relative feature importance for each feature and feature category.

## Results

### Sample characteristics


All 1086 participants who were randomized to treatment in the comparative effectiveness trial [@bakerEffectsNicotinePatch2016] were included in our analysis sample. Demographic characteristics of this sample appear in [@tbl-demohtml]{.content-visible when-format="html"}[@tbl-demopdf]{.content-visible unless-format="html"}. Smoking-related characteristics of this sample appear in @tbl-smoking-chars.

::: {.content-hidden unless-format="html"}

{{< embed notebooks/mak_tables.qmd#tbl-demohtml >}} 
:::

::: {.content-hidden when-format="html"}

{{< embed notebooks/mak_tables.qmd#tbl-demopdf >}} 
{{< pagebreak >}}   
 
[Table 2: Demographic Characteristics (Continued)]{.center}
{{< embed notebooks/mak_tables.qmd#demopdf >}} 
:::

{{< embed notebooks/mak_tables.qmd#tbl-smoking-chars >}} 

### Clinical benefit


There was a significant effect of treatment matching on abstinence at 4 weeks (OR = 1.382, *z* = 2.452, *p* = 0.014) such that individuals who received their model-predicted best treatment were more likely to be abstinent. The effect of treatment matching was no longer significant at 12 weeks (*p* = 0.232) or at the 26-week follow-up assessment (*p* = 0.943). @fig-clin-ben-wk4 shows the mean abstinence rate by treatment matching at each time point. The pattern of results was identical when including only individuals who reported any medication use in the analysis sample (4 weeks: OR = 1.359, *z* = 2.257, *p* = 0.024; 12 weeks: *p* = 0.295; 26 weeks: *p* = 0.902).

{{< embed notebooks/eval_benefit_4wk.qmd#fig-clin-ben-wk4 >}} 

Clinical benefit fairness analysis results here! ^[Fifty-three participants (4.9%) did not provide income data and were excluded from this analysis.]
<!--  i think you just need these?  these are second most important analyses -->

### Model performance


The median auROC across the 30 test sets for the 4-week model was 0.695 (IQR = 0.667 - 0.718, range = 0.592 - 0.788). @fig-combined, Panel A shows the ROC curve for held-out test set performance (concatenated across 30 held-out folds).

{{< embed notebooks/mak_fig_1.qmd#fig-combined >}}

We used the 30 test set auROCs to estimate the posterior probability distribution for the auROC of these models. The median auROC from the posterior distribution was 0.693. This value represents our best estimate for the magnitude of the auROC parameter. The 95% Bayesian CI for the auROC was relatively narrow [0.674 - 0.711] and did not contain 0.5 (chance performance), providing strong evidence that this model has predictive signal. @fig-combined, Panel B displays the posterior probability distribution for the auROC.

In @fig-combined, panel C, we display our model's calibration. Predicted probabilities are binned (bin width = 10%) and plotted against the observed probability of abstinence for observations in that bin. If probabilities were perfectly calibrated, all bin means would fall on the dotted line (e.g., bin from 0 - 10 with an observed mean probability of 0.05, bin from 10 - 20 with an observed mean probability of 0.15). Probabilities are well calibrated and ordinal in their relationship with the true probability of abstinence. 

### Model interpretation


Our final model fit with the full dataset retained 155 features overall including 74 treatment interactions (see Supplemental Table X for retained feature list). To perform treatment selection, only interactive features need to be assessed, as features that increase or decrease probabilities equally across treatments do not help to make different predictions across candidate medications. Implementing this model for treatment selection requires assessing only 52 unique items (e.g., multiple one hot features are from a single item, the same feature interacts with more than one treatment). 

Global feature importance (mean |Shapley value|) from our model appears in @fig-shap-global. Shapley values describe the relative importance of these individual features for making predictions. Six of the top 25 most globally important features were treatment interactions. <!-- GEF: UPDATE FIGURE TO ONLY BE PANEL A -->

{{< embed notebooks/shap_4wk.qmd#fig-shap-global >}}

## Discussion


In this project, we produced a treatment selection model that can offer immediate benefit to individuals looking to quit smoking using several first-line medications. Individuals who received their model-predicted best treatment in the original trial had a mean abstinence rate that was 7.4% higher than individuals who did not (38.9% vs. 31.5%) at 4 weeks. Although this absolute difference may seem somewhat small, this corresponds to a 23.5% *relative* improvement - simply by allocating treatments to the right person.<!-- good!  -->

We feel confident in this effect for several reasons. First, we made predictions for each individual to identify their best treatment while they were held-out from model fitting to match how this model will be used in clinical practice (i.e., to make predictions and select a treatment for new patients). Second, our model is capturing predictive signal, as supported by the Bayesian CI around our model's auROC, adding to our trust in the model's prediction outputs. Third, these predictions were well-calibrated such that we can trust their ordinal ranking. This is critical because our treatment selection process relies on the relative *order* (i.e., rank) of predicted probabilities for each person rather than the values themselves. 
  
We can achieve this benefit using an accessible, low-burden assessment. Implementing this treatment selection model would require assessing approximately 50 multiple choice and yes/no questions, which survey research suggests would take 11-12 minutes to complete [@lenznerCognitiveBurdenSurvey2010]. Additionally, because all items are self-report questions, this assessment can be completed remotely (e.g., administered online) and can be made available to people without access to in-person medical care. This remote capacity is particularly valuable because two treatments in the model (C-NRT, nicotine patch) are widely available over-the-counter, offering scalable implementation when healthcare access is limited. 

<!--  will need to synch this next paragraph with fairness for clinical benefit -->
This focus on accessibility is especially important given disparities in mental healthcare. Access to treatment is a known barrier in mental healthcare and a contributing factor driving healthcare disparities [@jacobsonDigitalTherapeuticsMental2022]. Cigarette smoking rates remain higher in many marginalized populations [@bakerSmokingTreatmentReport2021; @jamalCurrentCigaretteSmoking2015a; @corneliusTobaccoProductUse2020; @kellyPrevalenceSmokingOther2012; @cropseySmokingFemalePrisoners2004; @harrisonCigaretteSmokingMental2020; @baggettTobaccoUseHomeless2013; @soarSmokingAmongstAdults2020]. Prioritizing accessibility in implementation is a critical first step towards mitigating rather than exacerbating health disparities [@maceachernMachineLearningPrecision2021]. 

We further addressed this goal with our fairness benchmark analyses. (CLINICAL BENEFIT RESULTS DISCUSSION HERE - MAY NEED TO ADJUST CONCLUSIONS). These findings suggest...  <!-- [from original discussion of model performance] our model may be well-positioned to improve treatment equity. If our goal is to reduce and ultimately close the treatment gap between privileged individuals and individuals often disadvantaged in research efforts and mental healthcare, then we need tools such as this model that offer *additional* benefit to those disadvantaged demographic groups. -->

### Long-term outcomes

#### Challenges of predicting distal treatment success


Alongside these exciting findings, this study also demonstrates that it is difficult to predict complex outcomes like treatment success using distal predictors. Our best estimate of median auROC in held-out data (from the posterior probability distribution) was 0.69, indicating that our model correctly assigns a higher probability to a positive (abstinent) case than a negative (smoking) case 69% of the time. An auROC of 0.7 to 0.8 is considered "acceptable" [@mandrekarReceiverOperatingCharacteristic2010]. Our model was right on the cusp of this range. Nevertheless, there is room for improvement in our model's predictive performance. 

It is important to note, however, that our primary goal was *not* overall prediction of treatment success. Capturing some signal with our model was necessary to yield credible predictions and establish the relevance of features, and so our model's modest predictive performance may have impacted our ability to select treatments. However, overall prediction was insufficient for our purpose. A model that predicted treatment success perfectly (i.e., auROC of 1.0) but included no interactions would be useless for treatment selection. Indeed, this fact motivated our use of a satisficing metric for a minimum number of interaction terms retained in our models, even though it is possible that we sacrificed some level of predictive power by imposing that additional criterion.

#### Improving long-term benefit of treatment selection
  
<!-- first again?  -->
The clinical benefit that our treatment selection model offers is short-lived. There is no longer statistically significant benefit of treatment matching at 12 weeks, though there is a numeric difference (31.2% abstinence vs. 27.8% abstinence), and there is no statistical or numeric difference at all by 6 months. 

Initial treatment success is critical, especially for cigarette smoking where even reducing smoking or quitting for a short period of time can improve health outcomes and life expectancy [@jhaprabhat21stCenturyHazardsSmoking2013]. Additionally, smoking early in a quit attempt can have strong negative consequences: decreased self-efficacy, reduced treatment adherence, and premature treatment cessation [@schlamInterventionsTobaccoSmoking2013]. Findings such as these highlight that selecting a treatment that increases treatment success in early recovery (i.e., at 4 weeks) is necessary - though not sufficient - for long-term success.

Nevertheless, we were of course hopeful that treatment matching benefits would endure. However, it is perhaps unsurprising that they do not. There may be several possibilities for the lack of benefit at later assessment points. 

First, it is possible that we could have improved prediction overall and treatment selection if we incorporated biological markers or genetic features [@chenPathwaysPrecisionMedicine2018]. However, extant literature has primarily used single candidate genes or biomarkers, has also not found long-term benefits, and has not translated well into real-world settings [@chenGeneticVariantCHRNA52020; @shahabDoesNicotineMetabolite2019; @schnollNicotineMetabolicRate2009; @glatardAssociationNicotineMetabolism2017; @chenowethNicotineMetaboliteRatio2016; @lermanUseNicotineMetabolite2015; @siegelUseNicotineMetabolite2020]. Additionally, the potential improvement that could come from including biological or genetic features carries an associated cost to implementation given the relative inaccessibility of genetic and biological testing [@maceachernMachineLearningPrecision2021]. 

A second possibility is that we failed to include non-biological/non-genetic features that are critical for predicting treatment success later on. However, our data come from a large comparative effectiveness trial conducted by a nationally recognized center and designed by foremost experts in the field [@bakerEffectsNicotinePatch2016]. The baseline assessment was quite comprehensive and was based on domain expertise and decades of research. Thus, it seems unlikely that we could be missing enough important features to bridge the gap in our benefit of treatment selection.

<!--KW: You mention Bupropion later on. It makes me wonder if perhaps a possibility for lack of long term effects is not just related to features but also to the type of treatments you are matching people too. For example, maybe nicotine replacement treatments are more effective in the immediate transition from smoking, but something like bupropion or other treaments are better to switch to later after that initial transition for maintenance.

GEF: This is an interesting idea. I'm not opposed to working it in here, but I'm not quite sure how to incorporate it. Thoughts? 

jc- good idea
more tx by diff mech allows for having a best for everyone and therefore better benefit overall and maybe at later times

-->

We believe the most likely possibility is that our model did not capture dynamic changes over time. It may be that the same features predict treatment success across time. However, because these characteristics can change dynamically *within an individual*, what was the right treatment based on pre-quit characteristics is no longer the right treatment by 12 weeks, 6 months, or beyond. Many of the features used for prediction in this model were baseline measurements of *current* states - withdrawal, dependence, confidence/motivation to quit, time around other smokers, distress tolerance, depression symptoms, among others. Even features that feel more "static" like employment or marital status can be subject to change. 

Dynamic change is the rule rather than the exception when it comes to chronic diseases like substance use disorders. Tobacco and other substance use disorders are dynamic and relapsing; both risk for use and the factors driving that risk fluctuate over time [@brandonRelapseRelapsePrevention2007]. This change over time has been identified as a key barrier to overcome for precision mental health goals in addiction [@oliverPrecisionMedicineAddiction2017]. 

Accounting for dynamic changes will require ongoing assessment of key features that predict treatment success. Such monitoring is now feasible given developments in personal sensing (i.e., in situ data collection via sensors embedded in individuals' daily lives) [@epsteinPredictionStressDrug2020; @soysterPooledPersonspecificMachine2022; @chihPredictiveModelingAddiction2014; @baeMobilePhoneSensors2018; @wyantMachineLearningModels2023] and is acceptable to individuals with substance use disorders [@wyantAcceptabilityPersonalSensing2023]. Sensing via ecological momentary assessment is well-positioned to capture the self-report items used as features in this treatment selection model. Although frequently answering a 50-item survey is likely not feasible, it may be that more proximal features have greater predictive power, thus requiring fewer features for successful treatment selection. Indeed, in previous work in our laboratory, we have been able to predict hour-by-hour probabilities of alcohol lapses quite accurately (auROCs > 0.9) using features derived from only 10 self-report items that were measured much closer to the outcome [@wyantMachineLearningModels2023].

Using this dynamic monitoring, we can select and adapt treatments and supports over time. There are certainly opportunities to adapt medications – for example, titrating doses or identifying moments when someone could benefit most from a lozenge. In general, medications are more static treatments that are less well-suited to dynamic changes, but they do not need to be used on their own. We observed that matching people to the right medication improves treatment success at 4 weeks. Medications could help create early treatment success that positions people to engage with additional supports that might be able to adapt more dynamically. Alternative treatments and supports via web-based interventions and mobile health apps may offer platforms for sustainable, scalable, ongoing support. We can recommend specific modules and individual tools that map onto currently important features affecting treatment success. This mapping between risk factors and supports is likely to be quite complex and will require considerable future research. But if we hope to advance precision mental health for smoking, addiction, and even mental health broadly, we must consider the dynamic nature of risk and recovery inherent in these conditions.

### Interpreting our treatment selection model


Several recent reviews have noted that the low interpretability of "black box" machine learning models may impede their utility for clinical and public health goals [@maceachernMachineLearningPrecision2021; @mooneyBigDataPublic2018; @cohenTreatmentSelectionDepression2018]. Consequently, we aimed to make our model as interpretable as possible. We identified features that predict differential treatment success (i.e., features that interact with treatment to help us select among treatments). Our best selected model configuration retained 74 interaction terms that spanned 52 unique items. Each feature's associated global Shapley value, which indicates overall magnitude of feature importance, was relatively small. This finding supports what has long been suspected in precision mental health: There is no one feature that explains sufficient variance to make differential predictions by treatment on its own. Rather, each feature offers only a small contribution, but many features together can guide treatment selection.

We also identified features that predict treatment success overall (i.e., "main effects"). These features contribute to the smoking cessation literature and support the conclusion from a recent review that predictors of treatment success span many categories [@bickelPredictorsSmokingCessation2023]. We found similar breadth in important features in this model: economic (e.g., income), environmental (e.g., living with another smoker), sociodemographic (e.g., marital status), psychological (e.g., depression diagnosis), physical health (e.g., pain interfering with daily activities), and smoking use/history (e.g., longest previous quit attempt) characteristics all contributed to predicting treatment success. 

Additionally, these "main effect" features may yet help to advance precision mental health goals. These features may represent mechanisms underlying smoking cessation success and thus offer targeted areas for future treatment development. They also may be used to tailor existing treatments to increase success across individuals. Main effect features in our model may also interact with other treatments not included in this study. For example, buproprion is another first-line smoking cessation medication [@cahillPharmacologicalInterventionsSmoking2013]. It may be that some features in this model do not differentiate among C-NRT, nicotine patch, or varenicline but would differentiate between one of these treatments and buproprion. 

#### The role of demographic features

<!--  main or interaction? -->
Several demographic features emerged as important interactive features for treatment selection: race, gender, income, and marital status. Ethnicity did not emerge as an important interactive feature, and race-based interactive features were specifically related to identifying as a Black or White individual. However, the limited representation of Hispanic, Latino/a, Asian, Multiracial, and Native American/Alaska Native individuals in this sample warrants caution in drawing conclusions about the utility of ethnicity- and other race-based features for treatment selection. 

In some contexts, it would be problematic to use features that tap into constructs delineating marginalized identities such as race or socioeconomic status. For example, making decisions about who gets insurance (or doesn't) and who gets released earlier from incarceration (or doesn't) based on race is discriminatory (e.g., [@farayolaEthicsTrustworthinessAI2023]). However, in the precision mental health landscape, we are not deciding *who* gets treatment. Rather, we are deciding *which* treatment to give a specific patient. Thus, we can take advantage of experiential or symptomatological differences as a result of characteristics such as race, ethnicity, sex, income, or comorbid health conditions to improve treatment outcomes across vulnerable subpopulations.

#### The risk of transparency


When patients and clinicians can easily see non-intuitive features, they may be dissuaded from using or trusting this treatment selection model. For example, an interaction feature with the Anxiety Sensitivity Index-3 item "When my throat is tight, I worry I will choke to death" was retained. Because it seems to indicate a rather severe level of anxiety about physical conditions, perhaps it offered more predictive power than other correlated features and was thus retained by GLMNet. Regardless of the reason, the reality is that seeing questions like this on a treatment selection assessment may make patients or clinicians doubt the trustworthiness of the model.
<!--  if you only have panel a, do you see this.  most important are top features? -->

### Future directions


We made a concerted effort in this project to evaluate how our treatment selection model would perform for new patients using cross-validation procedures. Regardless, the ultimate test of this model's clinical benefit will be in a prospective trial. This trial will offer two tests. First, it will assess whether using this model is feasible and acceptable to patients and clinicians in clinical practice. Second, we can evaluate the benefit of our treatment selection model in an entirely new sample. Individuals who receive their model-predicted best treatment could be compared to any one of several possible comparison groups. Individuals in the comparison group could receive a random treatment assignment (mimicking clinical trials). Alternatively, they could receive clinician-assigned treatment to mirror traditional treatment selection (and best clinical practice). Another option is that we could compare to a simpler model. For example, there is evidence that treatment adherence is higher when people choose their preferred treatment [@cropseyPilotTrialVivo2017]. Thus, treatments in the comparison group could be assigned based on patient preference. Each comparison offers different advantages and disadvantages that should be considered thoughtfully when designing a prospective trial.

### Conclusion


Overall, this study has potential for immediate benefit to individuals looking to quit smoking. Our treatment selection model can improve the probability of abstinence during early recovery by a statistically significant and clinically meaningful margin. Moreover, it can do so with a relatively low-burden assessment that uses widely accessible features. This treatment selection model may serve as an initial tool embedded in a continuing care landscape where treatments are adapted dynamically over time. The ultimate test of our model will be in a prospective trial that assesses its feasibility, acceptability, and effectiveness in clinical practice. We are optimistic about the promise our model holds to improve the public health burden of cigarette smoking.
